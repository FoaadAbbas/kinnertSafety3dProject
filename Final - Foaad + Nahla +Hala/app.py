# -*- coding: utf-8 -*-
"""Finnal_Dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dlr3qwiszKAPMz_NqpnPAG_i8Aqh_S4g
"""

# CELL 2: Import Libraries and Check Dependencies
# ==============================================
"""
ğŸ“š CELL 2: IMPORT LIBRARIES
Run this cell to import all required libraries and check what's available.
"""

import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import re
import time
import requests
# Check what packages are available
print("ğŸ” Checking available packages...")

# ChromaDB
try:
    import chromadb
    CHROMADB_AVAILABLE = True
    print("âœ… ChromaDB: Available")
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âŒ ChromaDB: Not available (will use fallback)")

# SentenceTransformers
try:
    from sentence_transformers import SentenceTransformer
    TRANSFORMERS_AVAILABLE = True
    print("âœ… SentenceTransformers: Available")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âŒ SentenceTransformers: Not available (will use TF-IDF)")

# OpenAI (SDK ×”×—×“×©)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
    print("âœ… OpenAI (new SDK): Available")
except ImportError:
    OPENAI_AVAILABLE = False
    print("âŒ OpenAI SDK: Not available (will use template responses)")

# Gradio for interface
try:
    import gradio as gr
    GRADIO_AVAILABLE = True
    print("âœ… Gradio: Available")
except ImportError:
    GRADIO_AVAILABLE = False
    print("âŒ Gradio: Not available (will use simple interface)")

# Fallback imports
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

print("\nğŸ“‹ System Status:")
print(f"   Vector DB: {'ChromaDB' if CHROMADB_AVAILABLE else 'Simple Store'}")
print(f"   Embeddings: {'Transformer' if TRANSFORMERS_AVAILABLE else 'TF-IDF'}")
print(f"   Generation: {'OpenAI GPT' if OPENAI_AVAILABLE else 'Template'}")
print(f"   Interface: {'Gradio' if GRADIO_AVAILABLE else 'Simple'}")

print("\nğŸ¯ Ready for Cell 3!")

# CELL 3: Vector Store Classes
# ============================
"""
ğŸ—„ï¸ CELL 3: VECTOR STORE CLASSES
This cell defines the vector storage classes with fallback options.
"""

class SimpleVectorStore:
    """Fallback vector store when ChromaDB is not available"""

    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadatas = []
        self.ids = []
        print("ğŸ“¦ SimpleVectorStore initialized")

    def add(self, embeddings, documents, metadatas, ids):
        """Add documents to the store"""
        self.embeddings.extend(embeddings)
        self.documents.extend(documents)
        self.metadatas.extend(metadatas)
        self.ids.extend(ids)
        print(f"âœ… Added {len(documents)} documents to simple vector store")

    def query(self, query_embeddings, n_results=5):
        """Query the vector store"""
        if not self.embeddings:
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}

        # Calculate similarities
        similarities = cosine_similarity(query_embeddings, self.embeddings)[0]

        # Get top results
        top_indices = np.argsort(similarities)[::-1][:n_results]

        results = {
            'ids': [[self.ids[i] for i in top_indices]],
            'documents': [[self.documents[i] for i in top_indices]],
            'metadatas': [[self.metadatas[i] for i in top_indices]],
            'distances': [[1 - similarities[i] for i in top_indices]]
        }

        return results

    def count(self):
        """Get count of doc"""
        return len(self.documents)

print("âœ… Vector store classes defined!")
print("ğŸ“‹ Next: Run Cell 4 for RAG system core")

# CELL 4: RAG System Core
# =======================
"""
ğŸ§  CELL 4: RAG SYSTEM CORE
This cell defines the main EcologicalRAG class.
"""

class EcologicalRAG:
    """Main RAG system for ecological research papers"""

    def __init__(self, openai_api_key=None):
        print("ğŸŒŠ Initializing Ecological RAG System...")

        # Setup embedding model
        if TRANSFORMERS_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                self.use_transformers = True
                print("âœ… Loaded SentenceTransformer embeddings")
            except:
                self.use_transformers = False
                self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
                print("âš ï¸ Using TF-IDF embeddings (fallback)")
        else:
            self.use_transformers = False
            self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
            print("âš ï¸ Using TF-IDF embeddings")

        # Setup vector store
        if CHROMADB_AVAILABLE:
            try:
                client = chromadb.Client()
                try:
                    self.collection = client.get_collection("ecological_papers")
                    print("âœ… Loaded existing ChromaDB collection")
                except:
                    self.collection = client.create_collection("ecological_papers")
                    print("âœ… Created new ChromaDB collection")
                self.use_chromadb = True
            except:
                self.collection = SimpleVectorStore()
                self.use_chromadb = False
                print("âš ï¸ Using simple vector store (fallback)")
        else:
            self.collection = SimpleVectorStore()
            self.use_chromadb = False
            print("âš ï¸ Using simple vector store")

        # Setup OpenAI
        # Setup OpenAI (SDK ×”×—×“×©)
        if openai_api_key and OPENAI_AVAILABLE:
            import os
            os.environ["OPENAI_API_KEY"] = openai_api_key  # ×‘×¤×¨×•×“ ×œ× ×œ×©××•×¨ ××¤×ª×— ×‘×§×•×“
            try:
                # OpenAI ××•×‘× ×-CELL 2: from openai import OpenAI
                self.client = OpenAI()  # ×™×§×¨× ××ª ×”××¤×ª×— ××”-ENV
                self.use_openai = True
                print("âœ… OpenAI (new SDK) configured")
            except Exception as e:
                self.client = None
                self.use_openai = False
                print(f"âŒ OpenAI init failed: {e}")
        else:
            self.client = None
            self.use_openai = False
            print("âš ï¸ Using template responses")



        self.papers = []
        self.fitted = False
        print("ğŸ‰ RAG system ready!")

    def preprocess_text(self, text):
        """Clean text for better processing"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\-\.\(\)]', ' ', text)
        return text.strip()

    def extract_entities(self, text):
        """Extract ecological entities from text"""
        entities = {'species': [], 'locations': [], 'methods': []}

        # ---- Species (binomial) ----
        # ×“×•×¨×© ××‘× ×” Genus species: ××™×œ×” ×¨××©×•× ×” ×‘××•×ª ×’×“×•×œ×”, ×”×©× ×™×™×” ×‘××•×ª×™×•×ª ×§×˜× ×•×ª (>=3 ×ª×•×™×)
        species_matches = re.findall(r'\b([A-Z][a-z]{3,}\s[a-z]{3,})\b', text)

        # ××¡× × ×™× â€œ×–×™×•×¤×™×â€ × ×¤×•×¦×™× ×©× ×§×œ×˜×™× ×‘×˜×¢×•×ª ××”××××¨×™×
        bad_terms = {"The responses", "This thesis", "In the", "The seasonal", "In all"}
        species = [s for s in species_matches if s not in bad_terms]

        # × ×™×§×•×™ ×›×¤×™×œ×•×™×•×ª ×ª×•×š ×©××™×¨×ª ×¡×“×¨
        seen = set()
        species_clean = []
        for s in species:
            if s not in seen:
                seen.add(s)
                species_clean.append(s)
        entities['species'] = species_clean[:3]

        # ---- Locations ----
        locations = re.findall(
            r'\b(Mediterranean|Red Sea|Lake Kinneret|Kinneret|Sea of Galilee|Eastern Mediterranean|Levantine|exposed shores?)\b',
            text,
            re.IGNORECASE
        )
        entities['locations'] = list(dict.fromkeys([l.title() for l in locations]))[:3]

        # ---- Methods ----
        methods = re.findall(
            r'\b(PCR|DNA|sequencing|survey|analysis|model(?:ing)?|hindcast|nowcast|anemometer|wave gauge|station data)\b',
            text,
            re.IGNORECASE
        )
        entities['methods'] = list(dict.fromkeys([m.lower() for m in methods]))[:3]

        return entities


    def generate_embeddings(self, texts):
        """Generate embeddings using available method"""
        if self.use_transformers:
            return self.embedding_model.encode(texts, show_progress_bar=True)
        else:
            if not self.fitted:
                self.tfidf.fit(texts)
                self.fitted = True
            return self.tfidf.transform(texts).toarray()

print("âœ… RAG core class defined!")
print("ğŸ“‹ Next: Run Cell 5 for data loading methods")

# CELL 5: Data Loading Methods
# ============================
"""
ğŸ“š CELL 5: DATA LOADING METHODS
This cell adds data loading capabilities to the RAG system.
"""

def add_load_papers_method():
    """Add load_papers method to EcologicalRAG class"""

    def load_papers(self, papers_data):
        """Load papers into the RAG system"""
        print(f"ğŸ“š Loading {len(papers_data)} papers...")

        valid_papers = [p for p in papers_data if p.get('abstract', '').strip()]
        print(f"ğŸ“– Found {len(valid_papers)} papers with abstracts")

        if not valid_papers:
            print("âŒ No valid papers found!")
            return

        documents, metadatas, ids = [], [], []

        for i, paper in enumerate(valid_papers):
            # Combine title and abstract
            text = f"{paper.get('title', '')} {paper.get('abstract', '')}"
            text = self.preprocess_text(text)

            if len(text) < 50:
                continue

            entities = self.extract_entities(text)
            # Normalize year and clean url/doi
            year_raw = paper.get('year', 2022)
            try:
                year = int(str(year_raw).split()[0])
            except:
                year = 2022

            paper_url = str(paper.get('url', '')).strip()
            paper_doi = str(paper.get('doi', '')).strip()
            metadata = {
                'title': paper.get('title', 'Unknown'),
                'authors': paper.get('authors', 'Unknown'),
                'journal': paper.get('journal', 'Unknown'),
                'year': year,                 # ×‘××§×•× paper.get('year', 2022)
                'doi': paper_doi,             # ×‘××§×•× paper.get('doi', '')
                'url': paper_url,             # ×—×“×©
                'species': ', '.join(entities['species']),
                'locations': ', '.join(entities['locations']),
                'methods': ', '.join(entities['methods'])
            }

            documents.append(text)
            metadatas.append(metadata)
            ids.append(f"paper_{i}")

        if not documents:
            print("âŒ No processable documents found!")
            return

        # Generate embeddings
        print("ğŸ”„ Generating embeddings...")
        embeddings = self.generate_embeddings(documents)

        # Add to vector store
        print("ğŸ’¾ Adding to vector store...")
        if self.use_chromadb:
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
        else:
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

        self.papers = valid_papers
        print(f"âœ… Successfully loaded {len(documents)} papers!")

    # Add method to class
    EcologicalRAG.load_papers = load_papers

# Apply the method
add_load_papers_method()

print("âœ… Data loading methods added!")
print("ğŸ“‹ Next: Run Cell 6 for search and query methods")

# CELL 6: Search and Query Methods
# ================================
"""
ğŸ” CELL 6: SEARCH AND QUERY METHODS
This cell adds search and response generation to the RAG system.
"""

def add_search_methods():
    """Add search and query methods to EcologicalRAG class"""

    def search(self, query, n_results=3):
        """Search for relevant papers"""
        query_processed = self.preprocess_text(query)
        query_embedding = self.generate_embeddings([query_processed])

        if self.use_chromadb:
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=n_results
            )
        else:
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results
            )

        return results
    #000000
    def _generate_openai_response(self, query, papers, search_results):
        """Generate response using OpenAI (new SDK, text-only input)"""
        # ×‘×•× ×™× ×§×•× ×˜×§×¡×˜ ×§×¨×™×
        ctx_parts = []
        for i in range(min(len(papers), search_results.get("n_results", len(papers)))):
            title = papers[i].get('title', 'Untitled')
            authors = papers[i].get('authors', 'Unknown')
            full_doc = search_results['documents'][0][i]
            ctx_parts.append(f"Paper: {title}\nAuthors: {authors}\nContent: {full_doc}\n")
        context = "\n\n".join(ctx_parts)

        full_prompt = (
            "You are an expert marine and freshwater ecologist.\n"
            "Answer the user's question using ONLY the research context below. "
            "Cite the papers by title in-line when relevant.\n\n"
            f"Question:\n{query}\n\n"
            f"Research Context:\n{context}\n\n"
            "Answer:"
        )

        # DEBUG ×”×“×¤×¡×•×ª ×¢×•×–×¨×•×ª
        print("[RAG][DEBUG] use_openai =", self.use_openai, "client is None?", self.client is None)
        print("[RAG][DEBUG] prompt chars:", len(full_prompt))
        print("\n===== FULL PROMPT SENT TO GPT =====\n")
        print(full_prompt)
        print("\n===== END PROMPT =====\n")

        try:
            resp = self.client.responses.create(
                model="gpt-4.1-mini",
                input=full_prompt,          # ××—×¨×•×–×ª ××—×ª (×›××• ×‘Ö¾TEST)
                temperature=0.6,            # × ×•×¡×™×£ ×›×“×™ ×œ××¤×©×¨ ×™×¦×™×¨×ª×™×•×ª
                max_output_tokens=800
            )


            # ×‘××™×“×” ×•-output_text ×¨×™×§, × ×—×œ×¥ ×™×“× ×™×ª
            out = getattr(resp, "output_text", None)
            if out and out.strip():
                return out

            # ×—×™×œ×•×¥ ×–×”×™×¨ ××ª×•×š ×”××‘× ×” ×”×’× ×¨×™
            try:
                blocks = getattr(resp, "output", [])
                texts = []
                for b in blocks or []:
                    for c in getattr(b, "content", []) or []:
                        t = getattr(c, "text", None)
                        if t:
                            texts.append(getattr(t, "value", "") or str(t))
                joined = "\n".join([t for t in texts if t]).strip()
                if joined:
                    return joined
            except Exception as parse_e:
                print("[RAG][DEBUG] parse fallback error:", parse_e)

            print("[RAG][DEBUG] OpenAI response empty; falling back.")
            return self._generate_template_response(query, papers, search_results)

        except Exception as e:
            print("[RAG][DEBUG] OpenAI error:", repr(e))
            return f"OpenAI error: {e}\n\nFalling back to template response:\n\n{self._generate_template_response(query, papers, search_results)}"


    def _generate_template_response(self, query, papers, search_results):
        """Generate template response without OpenAI"""
        response = f"ğŸ” **Search Results for:** {query}\n\n"
        response += f"ğŸ“Š **Found {len(papers)} relevant papers:**\n\n"
        for i, paper in enumerate(papers[:search_results.get("n_results", len(papers))]):
            response += f"**{i+1}. {paper['title']}**\n"
            response += f"   ğŸ‘¥ Authors: {paper['authors']}\n"
            response += f"   ğŸ“– Journal: {paper['journal']} ({paper['year']})\n"

            if paper.get('species'):
                response += f"   ğŸŸ Species: {paper['species']}\n"
            if paper.get('locations'):
                response += f"   ğŸ“ Locations: {paper['locations']}\n"
            if paper.get('methods'):
                response += f"   ğŸ”¬ Methods: {paper['methods']}\n"

            # URL/DOI link (prefer URL if exists)
            if paper.get('url'):
                response += f"   ğŸ”— URL: {paper['url']}\n\n"
            elif paper.get('doi'):
                response += f"   ğŸ”— DOI: https://doi.org/{paper['doi']}\n\n"
            else:
                response += "\n"


        # Add summary
        all_species = set()
        all_locations = set()
        for paper in papers:
            if paper.get('species'):
                all_species.update([s.strip() for s in paper['species'].split(',') if s.strip()])
            if paper.get('locations'):
                all_locations.update([l.strip() for l in paper['locations'].split(',') if l.strip()])

        response += "ğŸ“‹ **Summary:**\n"
        if all_species:
            response += f"   ğŸŸ Species mentioned: {', '.join(list(all_species)[:5])}\n"
        if all_locations:
            response += f"   ğŸ“ Study areas: {', '.join(list(all_locations))}\n"

        return response

    def generate_response(self, query, search_results):
        """Generate response based on search results"""

        if not search_results['documents'][0]:
            return "âŒ No relevant papers found for your query."

        papers = search_results['metadatas'][0]

        if self.use_openai and getattr(self, "client", None):
            return self._generate_openai_response(query, papers, search_results)
        else:
            return self._generate_template_response(query, papers, search_results)


    def query(self, question, n_results=3):
        """Main query function"""
        print(f"ğŸ” Processing: {question}")

        search_results = self.search(question, n_results)
        search_results["n_results"] = n_results
        response = self.generate_response(question, search_results)


        return {
            'question': question,
            'response': response,
            'papers_found': len(search_results['documents'][0]),
            'search_results': search_results
        }

    # Add methods to class
    EcologicalRAG.search = search
    EcologicalRAG._generate_openai_response = _generate_openai_response
    EcologicalRAG._generate_template_response = _generate_template_response
    EcologicalRAG.generate_response = generate_response
    EcologicalRAG.query = query

# Apply the methods
add_search_methods()

print("âœ… Search and query methods added!")
print("ğŸ“‹ Next: Run Cell 7 for sample data")

# CELL 7: Sample Data
# ===================
"""
ğŸ“Š CELL 7: SAMPLE DATA
This cell provides sample IOLR papers for testing the system.
"""
"""

def get_sample_iolr_papers():
#×–×” ×“××•
    sample_papers = [
        {
            'title': 'The invasive silver-cheeked toadfish (Lagocephalus sceleratus) predominantly impacts the behavior of other non-indigenous species in the Eastern Mediterranean',
            'authors': 'Chaikin, S., De-Beer, G., Yitzhak, N., Stern, N., Belmaker, J.',
            'journal': 'Biological Invasions',
            'year': 2022,
            'doi': '10.1007/s10530-022-02972-7',
            'abstract': 'Invasive species can have cascading effects on marine ecosystems by altering the behavior and distribution of native species. We examined the impact of the invasive silver-cheeked toadfish (Lagocephalus sceleratus) on fish community behavior in the Eastern Mediterranean Sea. Our field surveys and behavioral experiments showed that the presence of this highly toxic species significantly altered feeding behavior, habitat use patterns, and predator-prey interactions of both native and other non-indigenous fish species. The toadfish acts as a keystone species that restructures marine food webs through its toxicity and aggressive behavior.'
        },
        {
            'title': 'Marine heatwaves drive recurrent mass mortalities in the Mediterranean Sea',
            'authors': 'Garrabou J, GÃ³mez-Gras D, Medrano A, Cerrano C, Ponti M, Rilov G',
            'journal': 'Global Change Biology',
            'year': 2022,
            'doi': '10.1111/gcb.16301',
            'abstract': 'Marine heatwaves have become increasingly frequent in the Mediterranean Sea, causing widespread mass mortality events across multiple taxonomic groups. We documented temperature anomalies reaching 5Â°C above long-term averages during summer 2022, leading to unprecedented mortality rates in benthic communities including corals, sponges, mollusks, and fish. These events affected shallow water habitats from 0-40m depth along the Mediterranean coast. Climate models predict increased frequency and intensity of such events, threatening the biodiversity and ecosystem services of Mediterranean marine ecosystems.'
        },
        {
            'title': 'Cyanobacterial pigment concentrations in inland waters: Novel semi-analytical algorithms for multi- and hyperspectral remote sensing data',
            'authors': 'Dev, P. J., Sukenik, A., Mishra, D. R., Ostrovsky, I.',
            'journal': 'Science of The Total Environment',
            'year': 2022,
            'doi': '10.1016/j.scitotenv.2021.150423',
            'abstract': 'Remote sensing of cyanobacterial blooms in freshwater systems requires accurate algorithms for pigment detection and quantification. We developed novel semi-analytical algorithms for detecting cyanobacterial pigments using multi-spectral and hyperspectral satellite data in Lake Kinneret, Israel. Our algorithms can distinguish between chlorophyll-a from cyanobacteria (phycocyanin) and other phytoplankton groups. Field validation using Sentinel-2 and PRISMA satellite imagery showed 85% accuracy in detecting Microcystis aeruginosa blooms. The algorithms provide early warning capabilities for harmful algal bloom management in freshwater ecosystems.'
        },
        {
            'title': 'Thermal vulnerability of the Levantine endemic and endangered habitat-forming macroalga, Gongolaria rayssiae: implications for reef carbon',
            'authors': 'Mulas, M., Silverman, J., Guy-Haim, T., Noe, S., Rilov, G.',
            'journal': 'Frontiers in Marine Science',
            'year': 2022,
            'doi': '10.3389/fmars.2022.862332',
            'abstract': 'Gongolaria rayssiae is an endemic brown macroalga forming extensive reefs along the Levantine coast. We assessed its thermal vulnerability through laboratory experiments and field monitoring during marine heatwaves. The species showed critical thermal limits at 32Â°C, with photosynthetic efficiency declining rapidly above 29Â°C. Field populations experienced 40% mortality during the 2022 heatwave when temperatures exceeded 30Â°C for extended periods. As a habitat-forming species and significant carbon sink, the loss of G. rayssiae reefs could have cascading effects on Mediterranean coastal carbon cycling and biodiversity.'
        },
        {
            'title': 'Trophic ecology of deep-sea megafauna in the ultra-oligotrophic Southeastern Mediterranean Sea',
            'authors': 'Guy-Haim, T., Stern, N., Sisma-Ventura, G.',
            'journal': 'Frontiers in Marine Science',
            'year': 2022,
            'doi': '10.3389/fmars.2022.857179',
            'abstract': 'The deep-sea ecosystems of the ultra-oligotrophic Southeastern Mediterranean Sea are poorly understood. We investigated the trophic ecology of megafauna using stable isotope analysis and stomach content examination from depths of 200-1000m. Fish, crustaceans, and cephalopods showed distinct trophic niches, with Î´15N values indicating 3-4 trophic levels. The food web relies heavily on marine snow and organic matter transport from surface waters. Despite low productivity, the deep-sea community maintains complex trophic relationships, with evidence of vertical migration connecting deep and shallow ecosystems.'
        },
        {
            'title': 'Jellyfish swarm impair the pretreatment efficiency and membrane performance of seawater reverse osmosis desalination',
            'authors': 'Rahav, E., Belkin, N., Nnebuo, O., Sisma-Ventura, G., Guy-Haim, T., Sharon-Gojman, R., Bar-Zeev, E.',
            'journal': 'Water Research',
            'year': 2022,
            'doi': '10.1016/j.watres.2022.118231',
            'abstract': 'Massive jellyfish blooms in the Eastern Mediterranean pose significant challenges to seawater desalination infrastructure. We investigated the impact of Rhopilema nomadica swarms on reverse osmosis (RO) membrane performance and pretreatment systems. Jellyfish biomass increased membrane fouling by 60% and reduced flux rates by 35%. Decomposing jellyfish released high concentrations of organic compounds that overwhelmed pretreatment filters. The study highlights the need for adaptive desalination strategies to cope with increasing jellyfish populations driven by climate change and overfishing in Mediterranean waters.'
        }
    ]

    print(f"ğŸ“š Loaded {len(sample_papers)} sample IOLR papers:")
    for i, paper in enumerate(sample_papers, 1):
        print(f"   {i}. {paper['title'][:60]}...")

    return sample_papers
"""
# Load sample data
#SAMPLE_PAPERS = get_sample_iolr_papers()

#print("\nâœ… Sample data ready!")
#print("ğŸ“‹ Next: Run Cell 8 to initialize RAG system")

# CELL 8: Initialize RAG System
# =============================
"""
ğŸš€ CELL 8: INITIALIZE RAG SYSTEM
This cell creates the RAG system and loads the sample papers.
Set your OpenAI API key here if you have one (optional).
"""

# Configuration
OPENAI_API_KEY = "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"  # Replace with your OpenAI API key if you have one
#OPENAI_API_KEY = None

#OPENAI_API_KEY = "sk-your-api-key-here"  # Uncomment and add your key

# Initialize the RAG system
print("ğŸŒŠ Initializing Ecological RAG System...")
rag_system = EcologicalRAG(openai_api_key=OPENAI_API_KEY)

# Load sample papers
#print("\nğŸ“š Loading sample papers into RAG system...")
#rag_system.load_papers(SAMPLE_PAPERS)

# Test the system
#print("\nğŸ§ª Testing system with sample query...")
#test_result = rag_system.query("What invasive species affect Mediterranean marine ecosystems?")
#print(f"âœ… Test successful! Found {test_result['papers_found']} relevant papers")

print("\nğŸ‰ RAG system is ready!")
print("ğŸ“‹ Next: Run Cell 9 for simple interface or Cell 10 to load your own papers")

# CELL 9: Simple Query Interface
# =============================
"""
ğŸ’¬ CELL 9: SIMPLE QUERY INTERFACE
This cell provides a simple interface to query the RAG system.
Copy and run this cell to start asking questions about ecology!
"""

def query_interface():
    """Simple interface for querying the RAG system"""

    print("ğŸŒŠ ECOLOGICAL RAG SYSTEM - QUERY INTERFACE")
    print("=" * 60)
    print("Ask questions about Lake Kinneret short-term wave and wind dynamics!")
    print("Type 'quit' to exit, 'help' for examples")
    print("=" * 60)

    while True:
        try:
            # Get user input
            query = input("\nğŸ” Your question: ").strip()

            if query.lower() == 'quit':
                print("ğŸ‘‹ Goodbye!")
                break

            if query.lower() == 'help':
                print("\nğŸ’¡ Example questions:")
                print("   â€¢ In what way does an increase in current wind speed affect wave height a few hours ahead?")
                print("   â€¢ How does the current wind direction influence wave height along exposed shores several hours into the future?")
                print("   â€¢ How does a rapid increase in wind intensity impact wave height in the near future?")
                print("   â€¢ How do wind speed and direction affect the ability to predict short-term wave height in Lake Kinneret?")
                print("   â€¢ In what way does an increase in wind speed influence wave height in Lake Kinneret?")
                print("   â€¢ How does wind direction affect drift intensity in Lake Kinneret?")
                continue

            if not query:
                print("âš ï¸ Please enter a question")
                continue

            # Process query
            print("\nğŸ”„ Searching through research papers...")
            result = rag_system.query(query, n_results=3)

            # Display results
            print("\n" + "="*60)
            print(f"ğŸ“‹ RESULTS FOR: {query}")
            print("="*60)
            print(result['response'])
            print("="*60)
            print(f"ğŸ“Š Found {result['papers_found']} relevant papers")
            print("ğŸ’¡ Type 'help' for more example questions")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("ğŸ’¡ Try a different question or check if the system is properly initialized")
            continue

def single_query(question):
    """Ask a single question without the interactive loop"""
    try:
        print(f"ğŸ” Searching for: {question}")
        result = rag_system.query(question, n_results=3)

        print("\n" + "="*60)
        print(f"ğŸ“‹ ANSWER:")
        print("="*60)
        print(result['response'])
        print("="*60)
        print(f"ğŸ“Š Based on {result['papers_found']} research papers")

        return result

    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

# Quick test of the interface
def test_interface():
    """Test the interface with sample questions"""

    test_questions = [
        "In what way does an increase in current wind speed affect wave height a few hours ahead?",
        "How do wind speed and direction affect the ability to predict short-term wave height in Lake Kinneret?",
        "How does wind direction affect drift intensity in Lake Kinneret?"
    ]

    print("ğŸ§ª Testing interface with sample questions...")

    for i, question in enumerate(test_questions, 1):
        print(f"\n[Test {i}/3] {question}")
        result = single_query(question)
        if result:
            print(f"âœ… Success!")
        else:
            print(f"âŒ Failed")

    print("\nâœ… Interface test completed!")

# Display available functions
print("âœ… Simple interface ready!")
print("\nğŸš€ Available functions:")
print("   â€¢ query_interface() - Start interactive questioning")
print("   â€¢ single_query('your question') - Ask one question")
print("   â€¢ test_interface() - Test with sample questions")

print("\nğŸ’¡ Example usage:")
print("   query_interface()  # Start interactive session")
print("   single_query('How do wind speed and direction affect the ability to predict short-term wave height in Lake Kinneret?')")

print("\nğŸ“‹ Next: Run Cell 10 to load your own papers (optional)")

# CELL 10: Load Your Own Papers (OPTIONAL)
# ========================================
"""
ğŸ“ CELL 10: LOAD YOUR OWN PAPERS (OPTIONAL)
Use this cell to load papers you collected with the scraper.
Skip this cell if you want to use the sample data.
"""

def load_collected_papers(file_path):
    """Load papers from your collected JSON file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            papers = json.load(f)

        # Filter papers with abstracts
        valid_papers = [p for p in papers if p.get('abstract', '').strip()]

        print(f"ğŸ“Š Loaded {len(papers)} total papers")
        print(f"âœ… Found {len(valid_papers)} papers with abstracts")

        return valid_papers

    except FileNotFoundError:
        print(f"âŒ File {file_path} not found")
        return None
    except Exception as e:
        print(f"âŒ Error loading papers: {e}")
        return None

def analyze_paper_collection(papers):
    """Analyze the loaded paper collection"""

    if not papers:
        print("âŒ No papers to analyze")
        return

    print("\nğŸ“Š PAPER COLLECTION ANALYSIS")
    print("="*50)

    # Basic stats
    total_papers = len(papers)
    with_abstracts = len([p for p in papers if p.get('abstract', '').strip()])

    print(f"ğŸ“š Total papers: {total_papers}")
    print(f"ğŸ“ With abstracts: {with_abstracts}")
    print(f"ğŸ“ˆ Success rate: {with_abstracts/total_papers*100:.1f}%")

    # Journal analysis
    journals = [p.get('journal', 'Unknown') for p in papers if p.get('journal')]
    if journals:
        journal_counts = pd.Series(journals).value_counts()
        print(f"\nğŸ“– Top journals:")
        for journal, count in journal_counts.head().items():
            print(f"   â€¢ {journal}: {count} papers")

    # Abstract length analysis
    abstract_lengths = [len(p.get('abstract', '')) for p in papers if p.get('abstract')]
    if abstract_lengths:
        print(f"\nğŸ“ Abstract lengths:")
        print(f"   â€¢ Average: {np.mean(abstract_lengths):.0f} characters")
        print(f"   â€¢ Range: {min(abstract_lengths)} - {max(abstract_lengths)}")

    print("="*50)

# UNCOMMENT THE LINES BELOW TO LOAD YOUR OWN PAPERS
"""
print("ğŸ“ Loading your collected IOLR papers...")

# Replace with your file path
your_papers = load_collected_papers('iolr_2022_abstracts_abstracts_only.json')

if your_papers:
    print(f"ğŸ”„ Replacing sample data with {len(your_papers)} collected papers...")

    # Analyze the collection
    analyze_paper_collection(your_papers)

    # Create new RAG system with your papers
    rag_system = EcologicalRAG(openai_api_key=OPENAI_API_KEY)
    rag_system.load_papers(your_papers)

    print("âœ… Your papers loaded successfully!")
else:
    print("âš ï¸ Could not load your papers, continuing with sample data")
"""

print("ğŸ“‹ Ready to load your own papers!")
print("Uncomment the code above and set your file path")
print("ğŸ“‹ Next: Run Cell 11 for Gradio interface (optional) or Cell 12 for analytics")

# CELL 10F: Load papers from Firebase (Realtime Database)
import requests

# === Your DB details (from the screenshot) ===
FIREBASE_BASE = "https://lakekinneretrag-default-rtdb.europe-west1.firebasedatabase.app"
PAPERS_PATH   = "lakeKinneret_research_papers"  # node containing the 5 papers
AUTH_TOKEN    = None  # if rules require auth, put idToken/secret here; otherwise leave None

def fetch_firebase_papers(base_url: str, path: str, auth: str | None = None):
    url = f"{base_url.rstrip('/')}/{path.strip('/')}.json"
    params = {"print": "pretty"}
    if auth:
        params["auth"] = auth

    r = requests.get(url, params=params, timeout=20)
    r.raise_for_status()
    data = r.json()

    if data is None:
        print("âš ï¸ No data at this path.")
        return []

    # supports both array [0..n] and dict {id: {...}}
    raw_list = list(data.values()) if isinstance(data, dict) else (data if isinstance(data, list) else [])
    papers = []
    for p in raw_list:
        if not isinstance(p, dict):
            continue
        title = str(p.get("title", "")).strip()
        abstract = str(p.get("abstract", "")).strip()
        if not title or not abstract:
            continue
        papers.append({
            "title": title,
            "authors": p.get("authors", "Unknown"),
            "journal": p.get("journal", "Unknown"),
            "year": p.get("year", 2022),
            "doi": str(p.get("doi", "")).strip(),
            "url": str(p.get("url", "")).strip(),
            "abstract": abstract
        })

    print(f"ğŸ“š Loaded {len(papers)} papers from Firebase")
    return papers

# Pull + load into the existing rag_system from Cell 8
firebase_papers = fetch_firebase_papers(FIREBASE_BASE, PAPERS_PATH, AUTH_TOKEN)
if firebase_papers:
    rag_system.load_papers(firebase_papers)
else:
    print("âš ï¸ No valid papers loaded from Firebase.")

# --- replace / update your build_rag_tab() like this ---

def build_rag_tab():
    import gradio as gr

    # NOTE: we scope everything under #rag-page and use rag-* class names
    css = """
    #rag-page { background:#f6f9fc; padding: 6px 10px 20px; }

    #rag-page .app-header {
        background: linear-gradient(90deg, #274060 0%, #3a6ea5 50%, #2d9cdb 100%);
        color:#fff; padding:26px 20px; border-radius:18px; margin-bottom:14px;
        text-align:center; box-shadow:0 4px 12px rgba(0,0,0,.15);
    }
    #rag-page .app-title { font-size:1.6rem; font-weight:700; margin:0; color:#ffffff; }
    #rag-page .app-sub   { opacity:.95; margin-top:8px; font-size:1rem; color:#f1f1f1; }

    /* reset any global borders/shadows inside the rag page */
    #rag-page .gr-row,
    #rag-page .gr-column,
    #rag-page .gr-group { border:none !important; box-shadow:none !important; }

    /* namespaced card */
    #rag-page .rag-card {
        background:#ffffff; border-radius:18px; padding:18px;
        box-shadow:0 6px 16px rgba(31,41,55,.08);
        border:1px solid #edf2f7; margin-bottom:16px;
    }

    /* namespaced pill button */
    #rag-page .rag-pill .gr-button {
        border-radius:999px !important; padding:14px 20px; font-weight:800; font-size:1.05rem;
        background:linear-gradient(135deg,#ff8a00,#ff6f3d) !important; color:#fff !important;
        box-shadow:0 10px 20px rgba(255,111,61,.25);
    }
    #rag-page .rag-pill .gr-button:hover { filter:brightness(.97); }

    /* namespaced answer card */
    #rag-page .rag-answer {
        background:#fdfefe; border-radius:18px; padding:20px;
        border:1px solid #e5ecf5; box-shadow:0 6px 16px rgba(31,41,55,.06);
        font-size:1rem; line-height:1.6;
    }

    #rag-page .papers-wrap { display:flex; flex-direction:column; gap:12px; }
    #rag-page .paper-card { background:#ffffff; border:1px solid #e9eef5; border-radius:14px; padding:14px; }
    #rag-page .paper-title { font-weight:700; color:#243647; margin-bottom:6px; }
    #rag-page .paper-meta { color:#5b6b7a; font-size:.9rem; margin-bottom:8px; }
    #rag-page .paper-snippet { color:#2c3e50; font-size:.95rem; }
    #rag-page .paper-actions a { text-decoration:none; }
    #rag-page .used-badge {
      display:inline-block; font-size:.78rem; padding:4px 8px; border-radius:999px;
      background:#e7f8ef; color:#1f7a4f; border:1px solid #c8efd9; margin-left:6px;
    }
    """

    def _format_papers_html(search_results, used_n=3):
        metas = (search_results or {}).get("metadatas", [[]])[0] or []
        docs  = (search_results or {}).get("documents", [[]])[0] or []
        cards = ['<div class="papers-wrap">']
        for i, meta in enumerate(metas):
            title   = meta.get("title", "Untitled")
            authors = meta.get("authors", "Unknown")
            journal = meta.get("journal", "Unknown")
            year    = meta.get("year", "")
            url     = (meta.get("url") or "").strip()
            doi     = (meta.get("doi") or "").strip()
            link = url or (f"https://doi.org/{doi}" if doi else "")
            snippet = (docs[i][:260] + "â€¦") if i < len(docs) and len(docs[i]) > 260 else (docs[i] if i < len(docs) else "")
            used_badge = '<span class="used-badge">Used in answer</span>' if i < used_n else ""
            cards.append(f"""
            <div class="paper-card">
              <div class="paper-title">{title} {used_badge}</div>
              <div class="paper-meta">Authors: {authors} &nbsp; | &nbsp; Journal: {journal} &nbsp; | &nbsp; Year: {year}</div>
              <div class="paper-snippet">{snippet}</div>
              <div class="paper-actions">{('<a href="'+link+'" target="_blank">Open</a>') if link else ""}</div>
            </div>
            """)
        cards.append("</div>")
        return "".join(cards)

    def _gradio_query(question, n_results=3):
        q = (question or "").strip()
        if not q:
            return {
                "response": "Please enter a short-term windâ€“wave question (Lake Kinneret).",
                "search_results": {"documents":[[]], "metadatas":[[]], "distances":[[]]},
            }
        try:
            return rag_system.query(q, n_results=int(n_results))
        except Exception as e:
            return {
                "response": f"Error: {e}",
                "search_results": {"documents":[[]], "metadatas":[[]], "distances":[[]]},
            }

    with gr.Group(visible=False, elem_id="rag-page") as page_rag:
        gr.HTML(f"<style>{css}</style>")

        with gr.Row():
            gr.HTML("""
            <div class="app-header">
              <p class="app-title">ğŸ” Academic RAG for Lake Kinneret</p>
              <p class="app-sub">Development of input and analysis of real-time data for short-term predictions of the state of the lake</p>
            </div>
            """)

        with gr.Group(elem_classes=["rag-card"]):
            question_in = gr.Textbox(
                label="ğŸ” Your research question",
                placeholder="e.g., How does wind speed affect short-term wave height in Lake Kinneret?",
                lines=2
            )

        examples = [
            ["In what way does an increase in current wind speed affect wave height a few hours ahead?", 3],
            ["How does the current wind direction influence wave height along exposed shores several hours into the future?", 3],
            ["How does a rapid increase in wind intensity impact wave height in the near future?", 3],
            ["How do wind speed and direction affect the ability to predict short-term wave height in Lake Kinneret?", 3],
            ["In what way does an increase in wind speed influence wave height in Lake Kinneret?", 3],
            ["How does wind direction affect drift intensity in Lake Kinneret?", 3],
        ]
        suggestion_choices = [row[0] for row in examples]

        with gr.Group(elem_classes=["rag-card"]):
            with gr.Row():
                with gr.Column():
                    n_papers = gr.Dropdown(
                        choices=[1, 2, 3, 4, 5], value=3, label="ğŸ“‘ Number of papers"
                    )
                with gr.Column():
                    sugg_dd = gr.Dropdown(
                        choices=["-- Select a question --"] + suggestion_choices,
                        value="-- Select a question --",
                        label="Suggested questions", allow_custom_value=False
                    )

            def _fill_q(choice, current_text):
                if choice is None or str(choice).startswith("--"):
                    return current_text
                return choice
            sugg_dd.change(fn=_fill_q, inputs=[sugg_dd, question_in], outputs=question_in)

        with gr.Group(elem_classes=["rag-pill"]):
            run_btn = gr.Button("Search & Write Answer", variant="primary")

        with gr.Group(elem_classes=["rag-card"]):
            answer_out = gr.Markdown(value="", label="Research-based Answer",
                                     elem_classes=["rag-answer"], visible=False)
            papers_out = gr.HTML(value="", visible=False)

        def _show_results(question, n_results):
            yield (
                gr.update(value="", visible=False),
                gr.update(value="", visible=False),
                gr.update(value="â³ Workingâ€¦", interactive=False)
            )
            k = int(n_results) if str(n_results).strip() else 3
            result = _gradio_query(question, k)
            sr = result.get("search_results", {})
            metas = (sr.get("metadatas") or [[]])[0] or []
            used_n = min(k, len(metas))
            papers_html = _format_papers_html(sr, used_n=used_n)
            yield (
                gr.update(value=result.get("response", ""), visible=True),
                gr.update(value=papers_html, visible=True),
                gr.update(value="Search & Write Answer", interactive=True)
            )

        run_btn.click(fn=_show_results, inputs=[question_in, n_papers],
                      outputs=[answer_out, papers_out, run_btn])

    return page_rag


# === PCA for SmartKinneret (read from GitHub) ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from urllib.parse import quote
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
plt.close('all')
# -------- 1) ×”×’×“×¨×ª ×××’×¨ ×•× ×ª×™×‘×™ ×§×‘×¦×™× ×‘×’×™×˜×”××‘ --------
USER   = "NahlaAboromi"
REPO   = "ecological-modeling-lab-team-biodynamics"
BRANCH = "main"

FILES = {
    "meto":    "Meto_Zemah2024_hourly (2).csv",
    "waves":   "KNW_ALL_Waves_merged (1).csv",
    "currents": "KNW_ALL_Currents_merged (2)_hourly_recalculated.csv",
}

def raw_url(path):
    return f"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{quote(path)}"

# -------- 2) ×§×¨×™××” ××”××™× ×˜×¨× ×˜ (GitHub Raw) --------
meto     = pd.read_csv(raw_url(FILES["meto"]))
waves    = pd.read_csv(raw_url(FILES["waves"]))
currents = pd.read_csv(raw_url(FILES["currents"]))

# -------- 3) ×”××¨×ª ×–××Ÿ ×•××—×™×“×•×ª ×œ×¨×–×•×œ×•×¦×™×™×ª ×©×¢×” --------
def coerce_time(df, candidates=("time", "date", "Date", "Time")):
    for c in candidates:
        if c in df.columns:
            out = df.copy()
            out["time"] = pd.to_datetime(out[c], errors="coerce")
            out = out.dropna(subset=["time"])
            out["time"] = out["time"].dt.floor("h")
            return out
    raise ValueError("×œ× × ××¦××” ×¢××•×“×ª ×–××Ÿ ××ª××™××” (time/date).")

meto     = coerce_time(meto)
waves    = coerce_time(waves)
currents = coerce_time(currents)

# -------- 4) ×‘×—×™×¨×ª ×¢××•×“×•×ª ×¨×œ×•×•× ×˜×™×•×ª --------
meto = meto[["time", "Ws_Z", "Wd_Z"]].drop_duplicates(subset=["time"]).copy()
waves = waves[["time", "Hs (m)"]].drop_duplicates(subset=["time"]).copy()
currents = currents[["time", "Velocity E [cm/s]", "Velocity N [cm/s]", "Magnitude [cm/s]"]].drop_duplicates(subset=["time"]).copy()

# -------- 5) ×—×™×©×•×‘ ×¨×›×™×‘×™ ×¨×•×— U/V (from bearing) --------
meto["Wd_Z_rad"] = np.deg2rad(meto["Wd_Z"])
meto["U_wind"]   = meto["Ws_Z"] * np.sin(meto["Wd_Z_rad"])  # EW
meto["V_wind"]   = meto["Ws_Z"] * np.cos(meto["Wd_Z_rad"])  # NS

# -------- 6) ×—×™×ª×•×š ×–×× ×™× ×—×•×¤×£ ×•××™×–×•×’ --------
common = set(meto["time"]).intersection(waves["time"]).intersection(currents["time"])
df = (meto[meto["time"].isin(common)]
      .merge(waves[waves["time"].isin(common)], on="time", how="inner")
      .merge(currents[currents["time"].isin(common)], on="time", how="inner")
      .sort_values("time").reset_index(drop=True))

print(f"âœ… ×©×¢×•×ª ×‘×—×™×ª×•×š: {len(df):,}")
print(f"×˜×•×•×—: {df['time'].min()} â†’ {df['time'].max()}")

# -------- 7) PCA --------
pca_vars = ["Ws_Z","Wd_Z","U_wind","V_wind","Hs (m)",
            "Velocity E [cm/s]","Velocity N [cm/s]","Magnitude [cm/s]"]

X = df[pca_vars].dropna().copy()
time_aligned = df.loc[X.index, "time"].reset_index(drop=True)

scaler = StandardScaler()
Xz = scaler.fit_transform(X)

pca = PCA()
Z   = pca.fit_transform(Xz)

expl = pca.explained_variance_ratio_*100
cum  = np.cumsum(expl)

loadings = pd.DataFrame(pca.components_.T, index=pca_vars,
                        columns=[f"PC{i+1}" for i in range(len(pca_vars))])

# -------- 8) Scree Plot --------
plt.figure(figsize=(10,6))
plt.bar(range(1,len(expl)+1), expl, alpha=0.9)
plt.plot(range(1,len(expl)+1), cum, marker="o")
plt.axhline(80, ls="--", lw=1, alpha=0.5)
plt.xlabel("Principal Component"); plt.ylabel("Explained Variance (%)")
plt.title("Scree Plot (Explained & Cumulative)")
plt.xticks(range(1,len(expl)+1)); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()

# -------- 9) Biplot --------
pc1, pc2 = 0, 1
plt.figure(figsize=(12, 9))
plt.scatter(Z[:, pc1], Z[:, pc2], s=14, alpha=0.6, zorder=1)

# ×ª×–×•×–×” ×™×“× ×™×ª ×œ×›×™×ª×•×‘×™× ×‘×¢×™×™×ª×™×™×  â† ×›××Ÿ!
label_offsets = {
    "Hs (m)": (-0.12, -0.10),
    "Velocity N [cm/s]": (0.10, 0.08),
}

scale = 3.3
for i, var in enumerate(pca_vars):
    x = loadings.iloc[i, pc1] * scale
    y = loadings.iloc[i, pc2] * scale
    plt.arrow(0, 0, x, y, head_width=0.06, length_includes_head=True, zorder=2)
    dx, dy = label_offsets.get(var, (0, 0))
    plt.text(x*1.08+dx, y*1.08+dy, var, fontsize=11, ha="center", va="center", zorder=3,
             bbox=dict(facecolor="white", edgecolor="none", alpha=0.85, boxstyle="round,pad=0.15"))

plt.axhline(0, lw=0.7); plt.axvline(0, lw=0.7)
plt.xlabel(f"PC1 ({expl[pc1]:.1f}%)"); plt.ylabel(f"PC2 ({expl[pc2]:.1f}%)")
plt.title("PCA Biplot"); plt.tight_layout(); plt.show()


# ===== U_wind Kriging from GitHub (no resampling) =====
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyproj import Transformer
from pykrige.ok import OrdinaryKriging
from pykrige.uk import UniversalKriging
from mpl_toolkits.axes_grid1 import make_axes_locatable

# --- 1) ××§×•×¨×•×ª GitHub (RAW) ---
SRC = {
    "Ein_Gev":  "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Ein_Gev2022-2024.csv",
    "Bteha":    "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Bteha2024.csv",
    "Ginosar":  "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Ginosar2024.xls",
    "Zemah":    "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Zemah2024.csv",
}

# --- 2) ×§×•××•×¨×“×™× ×˜×•×ª ×”×ª×—× ×•×ª (lat, lon) ---
stations_latlon = {
    "Bteha":   (32.887, 35.624),
    "Zemah":   (32.706, 35.577),
    "Ginosar": (32.861, 35.520),
    "Ein_Gev": (32.777, 35.646),
}

# --- 3) ×˜×•×•×— ×”×–××Ÿ ×”××©×•×ª×£ ×©× ×ª×ª ---
START = pd.Timestamp("2024-01-01 09:00")
END   = pd.Timestamp("2024-07-10 10:00")

# --- 4) ×¢×–×¨: ×–××Ÿ + ×—×™×©×•×‘ U ---
def parse_dt(s, dayfirst=True):
    return pd.to_datetime(s.astype(str), errors="coerce", dayfirst=dayfirst)

def uv_from_ws_wd(ws, wd_deg):
    theta = np.deg2rad(pd.to_numeric(wd_deg, errors="coerce").to_numpy())
    ws = pd.to_numeric(ws, errors="coerce").to_numpy()
    U = -ws * np.sin(theta)   # FROM â†’ TO  (×—×™×•×‘×™ ×œ××–×¨×—)
    V = -ws * np.cos(theta)   # ×—×™×•×‘×™ ×œ×¦×¤×•×Ÿ (×œ× × ×“×¨×© ×›××Ÿ)
    return U, V

def assert_strict_10min(tidx, name):
    if not (((tidx.second == 0) & (tidx.microsecond == 0) & (tidx.minute % 10 == 0)).all()):
        raise ValueError(f"{name}: ×—×•×ª××•×ª ×©××™× ×Ÿ 10 ×“×³ ×¢×’×•×œ×•×ª")
    if tidx.duplicated().any():
        raise ValueError(f"{name}: ×›×¤×™×œ×•×™×•×ª ×–××Ÿ")

# --- 5) ×˜×¢×™× ×ª ×ª×—× ×” (××”-GitHub, ×‘×œ×™ ×¨×™×¡××¤×œ×™× ×’) ---
def load_station(name):
    url = SRC[name]
    if url.lower().endswith(".csv"):
        df = pd.read_csv(url, low_memory=False)
    else:
        df = pd.read_excel(url, engine="xlrd")

    if "DateTime" not in df.columns:
        raise ValueError(f"{name}: ×œ× × ××¦××” ×¢××•×“×” 'DateTime' ××—×¨×™ ×”××™×—×•×“")

    # ×¤×¨×¡×™× ×’ ×¢× dayfirst; ×¢×•×‘×“ ×’× ×¢× ×•×‘×œ×™ ×©× ×™×•×ª
    t = pd.to_datetime(df["DateTime"], errors="coerce", dayfirst=True)

    # × ×¨××•×œ ×œ×¨×©×ª ×©×œ 10 ×“×§×•×ª (×™×‘×˜×™×— ××™× ×˜×¨×¡×§×©×Ÿ × ×§×™ ×‘×™×Ÿ ×›×œ ×”×ª×—× ×•×ª)
    t = t.dt.floor("10min")

    # ×©××•×ª ××”×™×¨×•×ª/×›×™×•×•×Ÿ ×¨×•×— × ×©××¨×™× ×›××• ×‘××§×•×¨ ×œ×›×œ ×ª×—× ×”:
    if name == "Ein_Gev":
        ws = df["WS_ms_Avg"];  wd = df["WindDir"]
    elif name == "Bteha":
        ws = df["ws_B"];       wd = df["Wd"]
    elif name == "Ginosar":
        ws = df["WS_ms_Avg"];  wd = df["WindDir"]
    elif name == "Zemah":
        ws = df["Ws_Z"];       wd = df["Wd_Z"]
    else:
        raise ValueError("Unknown station")


    out = pd.DataFrame({"time": t, "Ws": ws, "Wd": wd}).dropna().sort_values("time")
    # ×‘×“×™×§×ª 10 ×“×³ ×¢×’×•×œ×•×ª ×•×œ×œ× ×›×¤×™×œ×•×™×•×ª
    assert_strict_10min(pd.DatetimeIndex(out["time"]), name)
    # ×—×™×©×•×‘ U
    U, V = uv_from_ws_wd(out["Ws"], out["Wd"])
    out["U"] = U; out["V"] = V
    # ×¡×™× ×•×Ÿ ×œ×˜×•×•×— ×”××©×•×ª×£ ×‘×œ×‘×“
    return out[(out["time"] >= START) & (out["time"] <= END)].reset_index(drop=True)

# --- 6) ×˜×•×¢× ×™× ××ª 4 ×”×ª×—× ×•×ª ×•××•×¦××™× ××™× ×˜×¨×¡×§×©×Ÿ ---
st = {n: load_station(n) for n in ["Ein_Gev","Bteha","Ginosar","Zemah"]}
idxs = [pd.DatetimeIndex(st[n]["time"]) for n in st]
common = idxs[0].intersection(idxs[1]).intersection(idxs[2]).intersection(idxs[3])
if len(common) == 0:
    raise RuntimeError("××™×Ÿ ×—×•×ª××•×ª ×–××Ÿ ××©×•×ª×¤×•×ª ×‘×˜×•×•×— ×©× ×‘×—×¨")

print(f"×˜×•×•×— ××©×•×ª×£: {common[0]} â†’ {common[-1]} | {len(common)} ×—×•×ª××•×ª (×›×œ 10 ×“×³)")

# ×—×•×ª××ª ×–××Ÿ ×œ×‘×™×¦×•×¢ ×”×§×¨×™×’×™× ×’ (××¤×©×¨ ×œ×”×—×œ×™×£ ×œ×›×œ ×—×•×ª××ª ××ª×•×š common)
SELECT_TIME = pd.Timestamp("2024-06-10 13:10")
print(f"×–××Ÿ × ×‘×—×¨ ×œ×§×¨×™×’×™× ×’: {SELECT_TIME}")


# --- 7) ×¢×¨×›×™ U ×‘×ª×—× ×•×ª ×‘×–××Ÿ ×”× ×‘×—×¨ ---
order = ["Bteha","Zemah","Ginosar","Ein_Gev"]
vals_U = []
for n in order:
    row = st[n].set_index("time").loc[SELECT_TIME]
    if isinstance(row, pd.DataFrame):  # ×œ×™×ª×¨ ×‘×™×˜×—×•×Ÿ
        row = row.iloc[0]
    vals_U.append(float(row["U"]))
vals_U = np.array(vals_U)

# --- 8) ×”×§×¨× ×” ×œ-UTM36N ×•×’×¨×™×“ ---
transformer = Transformer.from_crs(4326, 32636, always_xy=True)
xs, ys = [], []
for n in order:
    lat, lon = stations_latlon[n]
    x_m, y_m = transformer.transform(lon, lat)
    xs.append(x_m); ys.append(y_m)
xs = np.array(xs); ys = np.array(ys)

pad = 2000  # ××˜×¨ ××¡×‘×™×‘
gx = np.linspace(xs.min()-pad, xs.max()+pad, 220)
gy = np.linspace(ys.min()-pad, ys.max()+pad, 220)
XX, YY = np.meshgrid(gx, gy)

# --- 9) Kriging ---
OK = OrdinaryKriging(xs, ys, vals_U, variogram_model="spherical",
                     verbose=False, enable_plotting=False)
z_ok, sig_ok = OK.execute("grid", gx, gy)

UK = UniversalKriging(xs, ys, vals_U, variogram_model="spherical",
                      drift_terms=["regional_linear"],
                      verbose=False, enable_plotting=False)
z_uk, sig_uk = UK.execute("grid", gx, gy)

# --- 10) ×©×¨×˜×•×˜ ---
plt.figure(figsize=(14,10))

plt.subplot(221)
sc = plt.scatter(xs, ys, c=vals_U, s=120, edgecolor='k', cmap="viridis")
plt.colorbar(sc, label="U_wind (m/s)")
plt.title(f"U_wind ×‘×ª×—× ×•×ª â€” {SELECT_TIME}")
plt.axis("equal")

plt.subplot(222)
im1 = plt.contourf(XX, YY, z_ok, levels=24, cmap="viridis")
plt.scatter(xs, ys, c='k', s=20)
plt.colorbar(im1, label="U_wind (m/s)")
plt.title("Ordinary Kriging")

plt.subplot(224)
vmax = float(np.nanmax(np.abs(z_uk)))
im2 = plt.contourf(XX, YY, z_uk, levels=24, vmin=-vmax, vmax=vmax, cmap="RdBu_r")
plt.scatter(xs, ys, c='k', s=20)
plt.colorbar(im2, label="U_wind (m/s)")
plt.title("Universal Kriging (linear drift)")

plt.subplot(223)
imv = plt.contourf(XX, YY, sig_ok, levels=24, cmap="YlOrRd")
plt.scatter(xs, ys, c='k', s=20)
plt.colorbar(imv, label="Variance")
plt.title("OK variance")

plt.tight_layout()
plt.show()
# --- UK variance (×¤×™×’×•×¨×” × ×¤×¨×“×ª) ---
plt.figure(figsize=(7,6))
im_uv = plt.contourf(XX, YY, sig_uk, levels=24, cmap="YlOrRd")
plt.scatter(xs, ys, c='k', s=20)
plt.colorbar(im_uv, label="Variance")
plt.title(f"UK variance â€” {SELECT_TIME}")
plt.axis("equal")
plt.tight_layout()
# ×× ×¨×•×¦×™× ×’× ×œ×©××•×¨:
# plt.savefig("uk_variance.png", dpi=300)
plt.show()

# ===== CELL 1: Logic + plotting (OK/UK) =====
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyproj import Transformer
from pykrige.ok import OrdinaryKriging
from pykrige.uk import UniversalKriging
from mpl_toolkits.axes_grid1 import make_axes_locatable   # ×‘×©×‘×™×œ colorbar ×‘×¦×“

# --- 1) ××§×•×¨×•×ª GitHub ---
SRC = {
    "Ein_Gev":  "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Ein_Gev2022-2024.csv",
    "Bteha":    "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Bteha2024.csv",
    "Ginosar":  "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Ginosar2024.xls",
    "Zemah":    "https://raw.githubusercontent.com/NahlaAboromi/ecological-modeling-lab-team-biodynamics/main/Meto_Zemah2024.csv",
}

# --- 2) ×§×•××•×¨×“×™× ×˜×•×ª ×”×ª×—× ×•×ª ---
stations_latlon = {
    "Bteha":   (32.887, 35.624),
    "Zemah":   (32.706, 35.577),
    "Ginosar": (32.861, 35.520),
    "Ein_Gev": (32.777, 35.646),
}

START = pd.Timestamp("2024-01-01 09:00")
END   = pd.Timestamp("2024-07-10 10:00")

# --- ×¢×–×¨ ---
def uv_from_ws_wd(ws, wd_deg):
    theta = np.deg2rad(pd.to_numeric(wd_deg, errors="coerce").to_numpy())
    ws = pd.to_numeric(ws, errors="coerce").to_numpy()
    U = -ws * np.sin(theta)
    V = -ws * np.cos(theta)
    return U, V

def assert_strict_10min(tidx, name):
    if not (((tidx.second == 0) & (tidx.microsecond == 0) & (tidx.minute % 10 == 0)).all()):
        raise ValueError(f"{name}: ×—×•×ª××•×ª ×©××™× ×Ÿ 10 ×“×³ ×¢×’×•×œ×•×ª")
    if tidx.duplicated().any():
        raise ValueError(f"{name}: ×›×¤×™×œ×•×™×•×ª ×–××Ÿ")

def load_station(name):
    url = SRC[name]
    if url.lower().endswith(".csv"):
        df = pd.read_csv(url, low_memory=False)
    else:
        df = pd.read_excel(url, engine="xlrd")
    if "DateTime" not in df.columns:
        raise ValueError(f"{name}: ×œ× × ××¦××” ×¢××•×“×” 'DateTime'")
    t = pd.to_datetime(df["DateTime"], errors="coerce", dayfirst=True).dt.floor("10min")

    if name == "Ein_Gev":
        ws = df["WS_ms_Avg"];  wd = df["WindDir"]
    elif name == "Bteha":
        ws = df["ws_B"];       wd = df["Wd"]
    elif name == "Ginosar":
        ws = df["WS_ms_Avg"];  wd = df["WindDir"]
    elif name == "Zemah":
        ws = df["Ws_Z"];       wd = df["Wd_Z"]

    out = pd.DataFrame({"time": t, "Ws": ws, "Wd": wd}).dropna().sort_values("time")
    assert_strict_10min(pd.DatetimeIndex(out["time"]), name)
    U, V = uv_from_ws_wd(out["Ws"], out["Wd"])
    out["U"] = U; out["V"] = V
    return out[(out["time"] >= START) & (out["time"] <= END)].reset_index(drop=True)

# --- 3) ×˜×¢×Ÿ ××ª ×›×œ ×”×ª×—× ×•×ª ---
ORDER = ["Bteha","Zemah","Ginosar","Ein_Gev"]
ST = {n: load_station(n) for n in ORDER}
IDX = [pd.DatetimeIndex(ST[n]["time"]) for n in ORDER]
COMMON = IDX[0].intersection(IDX[1]).intersection(IDX[2]).intersection(IDX[3])
COMMON_LABELS = [ts.strftime("%d/%m/%Y %H:%M") for ts in COMMON]
LABEL2TS = dict(zip(COMMON_LABELS, COMMON))

# --- 4) ×”×§×¨× ×” + ×’×¨×™×“ ×¨×™×‘×•×¢×™ ---
_transformer = Transformer.from_crs(4326, 32636, always_xy=True)
xs, ys = [], []
for n in ORDER:
    lat, lon = stations_latlon[n]
    x_m, y_m = _transformer.transform(lon, lat)
    xs.append(x_m); ys.append(y_m)
xs, ys = np.array(xs), np.array(ys)

def build_grid_square(xs, ys, pad=2000, res=220):
    xmin, xmax = xs.min() - pad, xs.max() + pad
    ymin, ymax = ys.min() - pad, ys.max() + pad
    cx, cy = (xmin + xmax)/2, (ymin + ymax)/2
    span = max(xmax - xmin, ymax - ymin)
    xmin, xmax = cx - span/2, cx + span/2
    ymin, ymax = cy - span/2, cy + span/2
    gx = np.linspace(xmin, xmax, res)
    gy = np.linspace(ymin, ymax, res)
    XX, YY = np.meshgrid(gx, gy)
    return gx, gy, XX, YY

gx, gy, XX, YY = build_grid_square(xs, ys)

# --- 5) ×—×™×©×•×‘ ---
def _vals_U_at(ts: pd.Timestamp):
    vals = []
    for n in ORDER:
        row = ST[n].set_index("time").loc[ts]
        if isinstance(row, pd.DataFrame):
            row = row.iloc[0]
        vals.append(float(row["U"]))
    return np.array(vals, dtype=float)

def _compute_fields(ts: pd.Timestamp, variogram_model="spherical"):
    vals_U = _vals_U_at(ts)
    OK = OrdinaryKriging(xs, ys, vals_U, variogram_model=variogram_model,
                         verbose=False, enable_plotting=False)
    z_ok, sig_ok = OK.execute("grid", gx, gy)
    UK = UniversalKriging(xs, ys, vals_U, variogram_model=variogram_model,
                          drift_terms=["regional_linear"],
                          verbose=False, enable_plotting=False)
    z_uk, sig_uk = UK.execute("grid", gx, gy)
    return {"z_ok": z_ok, "sig_ok": sig_ok, "z_uk": z_uk, "sig_uk": sig_uk}

# --- 6) ×¢×•×–×¨ ×œ×¦×‘×¢ ---
def _add_right_cbar(fig, ax, im, label):
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="3.5%", pad=0.15)
    cb = fig.colorbar(im, cax=cax)
    cb.set_label(label)
    return cb

# --- 7) ×¦×™×•×¨ ×¤×™×’×•×¨×•×ª ×¨×™×‘×•×¢×™×•×ª ---
def fig_ok(ts):
    F = _compute_fields(pd.to_datetime(ts))
    fig, ax = plt.subplots(figsize=(7,7), dpi=120)
    im = ax.contourf(XX, YY, F["z_ok"], levels=24, cmap="viridis")
    ax.scatter(xs, ys, c="k", s=20)
    _add_right_cbar(fig, ax, im, "U_wind (m/s)")
    ax.set_title("Ordinary Kriging")
    ax.set_xlim(gx.min(), gx.max()); ax.set_ylim(gy.min(), gy.max())
    ax.set_aspect("equal", adjustable="box"); ax.set_box_aspect(1)
    return fig

def fig_ok_variance(ts):
    F = _compute_fields(pd.to_datetime(ts))
    fig, ax = plt.subplots(figsize=(7,7), dpi=120)
    im = ax.contourf(XX, YY, F["sig_ok"], levels=24, cmap="YlOrRd")
    ax.scatter(xs, ys, c="k", s=20)
    _add_right_cbar(fig, ax, im, "Variance")
    ax.set_title("OK variance")
    ax.set_xlim(gx.min(), gx.max()); ax.set_ylim(gy.min(), gy.max())
    ax.set_aspect("equal", adjustable="box"); ax.set_box_aspect(1)
    return fig

def fig_uk(ts):
    F = _compute_fields(pd.to_datetime(ts))
    vmax = float(np.nanmax(np.abs(F["z_uk"])))
    fig, ax = plt.subplots(figsize=(7,7), dpi=120)
    im = ax.contourf(XX, YY, F["z_uk"], levels=24, vmin=-vmax, vmax=vmax, cmap="RdBu_r")
    ax.scatter(xs, ys, c="k", s=20)
    _add_right_cbar(fig, ax, im, "U_wind (m/s)")
    ax.set_title("Universal Kriging (linear drift)")
    ax.set_xlim(gx.min(), gx.max()); ax.set_ylim(gy.min(), gy.max())
    ax.set_aspect("equal", adjustable="box"); ax.set_box_aspect(1)
    return fig

def fig_uk_variance(ts):
    F = _compute_fields(pd.to_datetime(ts))
    fig, ax = plt.subplots(figsize=(7,7), dpi=120)
    im = ax.contourf(XX, YY, F["sig_uk"], levels=24, cmap="YlOrRd")
    ax.scatter(xs, ys, c="k", s=20)
    _add_right_cbar(fig, ax, im, "Variance")
    ax.set_title("UK variance")
    ax.set_xlim(gx.min(), gx.max()); ax.set_ylim(gy.min(), gy.max())
    ax.set_aspect("equal", adjustable="box"); ax.set_box_aspect(1)
    return fig

# --- 8) ×¢×–×¨ ×œ×–×× ×™× ---
def get_common_labels(): return COMMON_LABELS
def label_to_timestamp(label: str): return LABEL2TS[label]


# ===== CELL 2: Quick TEST (no dashboard) =====
labels = get_common_labels()
prefer_label = "10/06/2024 13:10"
label = prefer_label if prefer_label in labels else labels[0]
ts = label_to_timestamp(label)

from IPython.display import display

display(fig_ok(ts))
display(fig_ok_variance(ts))
display(fig_uk(ts))
display(fig_uk_variance(ts))

# === Find rows at 10/07/2024 10:00 across all source files (GitHub) ===

import pandas as pd
from urllib.parse import quote

USER, REPO, BRANCH = "NahlaAboromi", "ecological-modeling-lab-team-biodynamics", "main"

def raw_url(path: str) -> str:
    return f"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{quote(path)}"

# ---- Files & how to parse their datetime ----
FILES = {
    # Meteo (land)
    "Ein_Gev2022-2024.csv":   {"path": "Ein_Gev2022-2024.csv",   "type": "csv",  "dt_col": ("DateTime",),                      "tz_note": ""},
    "Meto_Bteha2024.csv":     {"path": "Meto_Bteha2024.csv",     "type": "csv",  "dt_col": ("DateTime",),                      "tz_note": ""},
    "Meto_Ginosar2024.xls":   {"path": "Meto_Ginosar2024.xls",   "type": "xls",  "dt_col": ("DateTime","date","time"),         "tz_note": ""},
    "Meto_Zemah2024.csv":     {"path": "Meto_Zemah2024.csv",     "type": "csv",  "dt_col": ("DateTime",),                      "tz_note": ""},
    # Lake (water)
    "KNC04_Currents.csv":     {"path": "KNC04_Currents.csv",     "type": "csv",  "dt_col": ("Date",),                          "tz_note": ""},
    "KNW04_Currents.csv":     {"path": "KNW04_Currents.csv",     "type": "csv",  "dt_col": ("Date",),                          "tz_note": ""},
    "KNW04_Waves.xlsx":       {"path": "KNW04_Waves.xlsx",       "type": "xlsx", "dt_col": ("Date & Time (GMT)",),             "tz_note": "GMT"},
}

TARGET = pd.to_datetime("10/07/2024 10:00", dayfirst=True)  # dd/mm/yyyy HH:MM  (ignores seconds)
def parse_dates(series):
    s = series.astype(str)
    # ×§×•×“× × ×¡×” dd/mm/yyyy
    dt = pd.to_datetime(s, format="%Y-%m-%d %H:%M:%S", errors="coerce")

    # ×›×œ ××” ×©×™×¦× NaT â€“ × ×¡×” yyyy-mm-dd
    mask = dt.isna()
    if mask.any():
        dt2 = pd.to_datetime(s[mask], dayfirst=False, errors="coerce")
        dt.loc[mask] = dt2
    return dt

def read_any(url: str, ftype: str) -> pd.DataFrame:
    if ftype == "csv":
        return pd.read_csv(url)
    if ftype == "xls":
        return pd.read_excel(url, engine="xlrd")
    if ftype == "xlsx":
        return pd.read_excel(url, engine="openpyxl")
    raise ValueError(f"Unknown file type: {ftype}")

def ensure_when(df: pd.DataFrame, dt_cols: tuple) -> pd.Series:
    """
    Returns a pandas Series of parsed datetimes (naive), with seconds ignored later by flooring to minute.
    Priority:
      1) If 'DateTime' exists -> parse it
      2) Else if single dt_cols[0] exists -> parse it
      3) Else if ('date','time') exist -> combine them: f"{date} {time}"
    """
    # 1) Direct "DateTime"
    if "DateTime" in df.columns:
        s = df["DateTime"].astype(str)
        return pd.to_datetime(s, dayfirst=True, errors="coerce")
    # 2) Single datetime column if present
    for c in dt_cols:
        if c in df.columns and c.lower() not in ("date","time"):
            s = df[c].astype(str)
            return pd.to_datetime(s, dayfirst=True, errors="coerce")
    # 3) Combine date+time
    date_col = next((c for c in ("date","Date","DATE") if c in df.columns), None)
    time_col = next((c for c in ("time","Time","TIME") if c in df.columns), None)
    if date_col and time_col:
        s = (df[date_col].astype(str).str.strip() + " " + df[time_col].astype(str).str.strip())
        return pd.to_datetime(s, dayfirst=True, errors="coerce")
    # If all else fails, try first provided column
    if dt_cols and dt_cols[0] in df.columns:
        return parse_dates(df[dt_cols[0]])

    raise KeyError("No datetime columns found to parse")

results = {}

for fname, cfg in FILES.items():
    url = raw_url(cfg["path"])
    try:
        df = read_any(url, cfg["type"])
    except Exception as e:
        print(f"âš ï¸ Failed to read {fname}: {e}")
        results[fname] = pd.DataFrame()
        continue

    # build 'when' column
    try:
        when = ensure_when(df, cfg["dt_col"])
    except Exception as e:
        print(f"âš ï¸ Failed to parse datetime for {fname}: {e}")
        results[fname] = pd.DataFrame()
        continue

    # ignore seconds by comparing floored-to-minute
    when_min = when.dt.floor("min")
    mask = (when_min == TARGET)
    hits = df.loc[mask].copy()
    results[fname] = hits

# ---- Print summary & show each match ----
print("=== Matches at 10/07/2024 10:00 (dd/mm/yyyy HH:MM) ===")
for fname, hits in results.items():
    n = len(hits)
    print(f"\nâ€¢ {fname}: {n} row(s)")
    if n:
        display(hits)

# ===== Helpers: load data from GitHub + compute Home KPIs (English) =====
import math
import pandas as pd
from urllib.parse import quote

# --- GitHub source ---
USER, REPO, BRANCH = "NahlaAboromi", "ecological-modeling-lab-team-biodynamics", "main"
def raw_url(path: str) -> str:
    return f"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{quote(path)}"

# --- Files map (names must match repo files) ---
FILES = {
    # Meteo (land)
    "Ein_Gev2022-2024.csv": {"path": "Ein_Gev2022-2024.csv", "type": "csv",  "dt_col": ("DateTime",)},
    "Meto_Bteha2024.csv":   {"path": "Meto_Bteha2024.csv",   "type": "csv",  "dt_col": ("DateTime",)},
    "Meto_Ginosar2024.xls": {"path": "Meto_Ginosar2024.xls", "type": "xls",  "dt_col": ("DateTime","date","time")},
    "Meto_Zemah2024.csv":   {"path": "Meto_Zemah2024.csv",   "type": "csv",  "dt_col": ("DateTime",)},
    # Lake (water)
    "KNC04_Currents.csv":   {"path": "KNC04_Currents.csv",   "type": "csv",  "dt_col": ("Date",)},
    "KNW04_Currents.csv":   {"path": "KNW04_Currents.csv",   "type": "csv",  "dt_col": ("Date",)},
    "KNW04_Waves.xlsx":     {"path": "KNW04_Waves.xlsx",     "type": "xlsx", "dt_col": ("Date & Time (GMT)",)},
}
def parse_dates(series):
    s = series.astype(str)
    # ×§×•×“× × ×¡×” dd/mm/yyyy
    dt = pd.to_datetime(s, format="%Y-%m-%d %H:%M:%S", errors="coerce")

    # ×›×œ ××” ×©×™×¦× NaT â€“ × ×¡×” yyyy-mm-dd
    mask = dt.isna()
    if mask.any():
        dt2 = pd.to_datetime(s[mask], dayfirst=False, errors="coerce")
        dt.loc[mask] = dt2
    return dt

def _read_any(url: str, ftype: str) -> pd.DataFrame:
    if ftype == "csv":
        return pd.read_csv(url)
    if ftype == "xls":
        return pd.read_excel(url, engine="xlrd")
    if ftype == "xlsx":
        return pd.read_excel(url, engine="openpyxl")
    raise ValueError(ftype)

def _ensure_when(df: pd.DataFrame, dt_cols: tuple) -> pd.Series:
    """Return a pandas Series with parsed datetimes (seconds tolerated)."""
    # 1) DateTime column has priority
    if "DateTime" in df.columns:
        return pd.to_datetime(df["DateTime"].astype(str), dayfirst=True, errors="coerce")
    # 2) single datetime column
    for c in dt_cols:
        if c in df.columns and c.lower() not in ("date","time"):
            return pd.to_datetime(df[c].astype(str), dayfirst=False, errors="coerce")
    # 3) combine date + time
    date_col = next((c for c in ("date","Date","DATE") if c in df.columns), None)
    time_col = next((c for c in ("time","Time","TIME") if c in df.columns), None)
    if date_col and time_col:
        s = (df[date_col].astype(str).str.strip() + " " + df[time_col].astype(str).str.strip())
        return pd.to_datetime(s, dayfirst=True, errors="coerce")
    # 4) fallback
    if dt_cols and dt_cols[0] in df.columns:
        return parse_dates(df[dt_cols[0]])

    raise KeyError("No datetime columns to parse")

def load_all_data(target_label: str = "10/07/2024 10:00") -> dict:
    """Read all repo files, filter to the exact minute of target_label (seconds ignored)."""
    target = pd.to_datetime(target_label, dayfirst=True)
    out = {}
    for fname, cfg in FILES.items():
        url = raw_url(cfg["path"])
        try:
            df = _read_any(url, cfg["type"])
            when = _ensure_when(df, cfg["dt_col"])
            hits = df.loc[when.dt.floor("min") == target].copy()
        except Exception as e:
            print(f"âš ï¸ {fname}: {e}")
            hits = pd.DataFrame()
        out[fname] = hits
    return out

def _avg_or_none(vals):
    vals = [v for v in vals if pd.notna(v)]
    return float(sum(vals)/len(vals)) if vals else None

def _deg_to_vec(deg):
    rad = math.radians(deg)
    return math.cos(rad), math.sin(rad)

def _vec_to_deg(x, y):
    if x == 0 and y == 0:
        return None
    ang = math.degrees(math.atan2(y, x))
    if ang < 0: ang += 360
    return ang

def english_wind_dir(deg: float) -> str:
    """Return English compass direction name for degrees."""
    if deg is None or pd.isna(deg):
        return "Not available"
    # 16-point compass rose (every 22.5Â°)
    dirs = [
        "North", "North-Northeast", "Northeast", "East-Northeast",
        "East", "East-Southeast", "Southeast", "South-Southeast",
        "South", "South-Southwest", "Southwest", "West-Southwest",
        "West", "West-Northwest", "Northwest", "North-Northwest"
    ]
    idx = int(((deg % 360) + 11.25) // 22.5) % 16
    return dirs[idx]

def compute_home_kpis(target_label: str = "10/07/2024 10:00") -> dict:
    """Compute Home KPIs (all-English labels/strings) for the requested time."""
    data = load_all_data(target_label)

    # --- Wind speed (m/s) from land stations ---
    wind_vals = []
    if not data["Ein_Gev2022-2024.csv"].empty and "WS_ms_Avg" in data["Ein_Gev2022-2024.csv"].columns:
        wind_vals.append(pd.to_numeric(data["Ein_Gev2022-2024.csv"]["WS_ms_Avg"], errors="coerce").mean())
    if not data["Meto_Bteha2024.csv"].empty and "ws_B" in data["Meto_Bteha2024.csv"].columns:
        wind_vals.append(pd.to_numeric(data["Meto_Bteha2024.csv"]["ws_B"], errors="coerce").mean())
    if not data["Meto_Ginosar2024.xls"].empty and "WS_ms_Avg" in data["Meto_Ginosar2024.xls"].columns:
        wind_vals.append(pd.to_numeric(data["Meto_Ginosar2024.xls"]["WS_ms_Avg"], errors="coerce").mean())
    if not data["Meto_Zemah2024.csv"].empty and "Ws_Z" in data["Meto_Zemah2024.csv"].columns:
        wind_vals.append(pd.to_numeric(data["Meto_Zemah2024.csv"]["Ws_Z"], errors="coerce").mean())
    wind_speed = _avg_or_none(wind_vals)

    # --- Air temperature (Â°C) ---
    temp_vals = []
    if not data["Ein_Gev2022-2024.csv"].empty and "AirTC_Avg" in data["Ein_Gev2022-2024.csv"].columns:
        temp_vals.append(pd.to_numeric(data["Ein_Gev2022-2024.csv"]["AirTC_Avg"], errors="coerce").mean())
    if not data["Meto_Ginosar2024.xls"].empty and "AirTC_Avg" in data["Meto_Ginosar2024.xls"].columns:
        temp_vals.append(pd.to_numeric(data["Meto_Ginosar2024.xls"]["AirTC_Avg"], errors="coerce").mean())
    if not data["Meto_Zemah2024.csv"].empty and "Ta_Z" in data["Meto_Zemah2024.csv"].columns:
        temp_vals.append(pd.to_numeric(data["Meto_Zemah2024.csv"]["Ta_Z"], errors="coerce").mean())
    air_temp = _avg_or_none(temp_vals)

    # --- Relative humidity (%) ---
    rh_vals = []
    if not data["Meto_Bteha2024.csv"].empty and "Rh" in data["Meto_Bteha2024.csv"].columns:
        rh_vals.append(pd.to_numeric(data["Meto_Bteha2024.csv"]["Rh"], errors="coerce").mean())
    if not data["Meto_Ginosar2024.xls"].empty and "RH" in data["Meto_Ginosar2024.xls"].columns:
        rh_vals.append(pd.to_numeric(data["Meto_Ginosar2024.xls"]["RH"], errors="coerce").mean())
    if not data["Meto_Zemah2024.csv"].empty and "rh_Z" in data["Meto_Zemah2024.csv"].columns:
        rh_vals.append(pd.to_numeric(data["Meto_Zemah2024.csv"]["rh_Z"], errors="coerce").mean())
    rh = _avg_or_none(rh_vals)

    # --- Average wind direction (vector) ---
    wdir_vals = []
    if not data["Ein_Gev2022-2024.csv"].empty and "WindDir" in data["Ein_Gev2022-2024.csv"].columns:
        wdir_vals.append(pd.to_numeric(data["Ein_Gev2022-2024.csv"]["WindDir"], errors="coerce").mean())
    if not data["Meto_Bteha2024.csv"].empty and "Wd" in data["Meto_Bteha2024.csv"].columns:
        wdir_vals.append(pd.to_numeric(data["Meto_Bteha2024.csv"]["Wd"], errors="coerce").mean())
    if not data["Meto_Ginosar2024.xls"].empty and "WindDir" in data["Meto_Ginosar2024.xls"].columns:
        wdir_vals.append(pd.to_numeric(data["Meto_Ginosar2024.xls"]["WindDir"], errors="coerce").mean())
    if not data["Meto_Zemah2024.csv"].empty and "Wd_Z" in data["Meto_Zemah2024.csv"].columns:
        wdir_vals.append(pd.to_numeric(data["Meto_Zemah2024.csv"]["Wd_Z"], errors="coerce").mean())
    wind_dir = None
    if wdir_vals:
        vecs = [_deg_to_vec(d) for d in wdir_vals if pd.notna(d)]
        if vecs:
            x = sum(v[0] for v in vecs) / len(vecs)
            y = sum(v[1] for v in vecs) / len(vecs)
            wind_dir = _vec_to_deg(x, y)

    # --- Current intensity (m/s) from shallow depth (prefer 2m) ---
    def _current_at_station(df):
        if df.empty:
            return None
        df2 = df.copy()
        if "Depth" not in df2.columns:
            return None
        df2["Depth_num"] = pd.to_numeric(df2["Depth"], errors="coerce")
        df2 = df2.dropna(subset=["Depth_num"])
        if df2.empty:
            return None
        if (df2["Depth_num"] == 2).any():
            row = df2.loc[df2["Depth_num"] == 2].iloc[0]
        else:
            row = df2.sort_values("Depth_num", ascending=True).iloc[0]
        mag = pd.to_numeric(row.get("Magnitude [cm/s]"), errors="coerce")
        return float(mag) / 100.0 if pd.notna(mag) else None

    cur_knc = _current_at_station(data["KNC04_Currents.csv"])
    cur_knw = _current_at_station(data["KNW04_Currents.csv"])
    current_intensity = _avg_or_none([v for v in (cur_knc, cur_knw) if v is not None])

    # --- Wave height (m) from KNW waves ---
    wave_hs = None
    waves = data["KNW04_Waves.xlsx"]
    if not waves.empty and "Hs (m)" in waves.columns:
        wave_hs = float(pd.to_numeric(waves["Hs (m)"], errors="coerce").mean())

    # --- Safety status rules ---
    # Safe if all: Hs<0.5, Current<0.3, Wind<6  ; else Advisory
    safety_ok = True
    reasons = []
    if (wave_hs is not None) and (wave_hs >= 0.5):
        safety_ok = False
        reasons.append("High waves (Hs â‰¥ 0.5 m)")
    if (current_intensity is not None) and (current_intensity >= 0.3):
        safety_ok = False
        reasons.append("Strong currents (â‰¥ 0.3 m/s)")
    if (wind_speed is not None) and (wind_speed >= 6.0):
        safety_ok = False
        reasons.append("Strong winds (â‰¥ 6 m/s)")

    safety_status = "Safe for swimming and boating" if safety_ok else "Advisory"
    alert_text = "â€”" if safety_ok else " | ".join(reasons) if reasons else "Advisory conditions"

    # --- Strings ready for UI ---
    def fmt(val, unit=None, digits=2, dash="â€”"):
        if val is None or pd.isna(val):
            return dash
        if unit is None:
            return f"{val:.{digits}f}"
        return f"{val:.{digits}f} {unit}"

    wind_dir_name = english_wind_dir(wind_dir)
    wind_dir_str  = "Not available" if wind_dir is None else f"{wind_dir_name} {round(wind_dir)}Â°"

    kpis = {
        "wind_speed": wind_speed,                 # m/s
        "air_temp": air_temp,                     # Â°C
        "current_intensity": current_intensity,   # m/s
        "wave_hs": wave_hs,                       # m
        "wind_dir": None if wind_dir is None else round(wind_dir),  # degrees
        "rh": None if rh is None else round(rh),  # %
        "safety_status": safety_status,           # "Safe for swimming and boating" / "Advisory"
        "alert_text": alert_text,                 # English reasons or em dash
        # convenience strings:
        "wind_speed_str": fmt(wind_speed, "m/s"),
        "air_temp_str": fmt(air_temp, "Â°C"),
        "current_intensity_str": fmt(current_intensity, "m/s"),
        "wave_hs_str": fmt(wave_hs, "m"),
        "wind_dir_str": wind_dir_str,
        "rh_str": "â€”" if (rh is None) else f"{round(rh)} %",
        "is_safe": safety_ok,                     # boolean for UI logic
    }
    return kpis

# Easily change the time if needed:
HOME_TS_LABEL = "10/07/2024 10:00"

# --- Common "SmartKinneret Hero" (CSS + helper) ---

SMART_HERO_CSS = """
.sk-page { background:#f6f9fc; padding:6px 10px 20px; }
.sk-hero {
  color:#fff; padding:26px 20px; border-radius:18px; margin-bottom:14px;
  text-align:center; box-shadow:0 4px 12px rgba(0,0,0,.15);
  background: linear-gradient(90deg, #274060 0%, #3a6ea5 50%, #2d9cdb 100%);
}
.sk-hero.orange { background: linear-gradient(90deg, #7a2e0e 0%, #d97706 50%, #f59e0b 100%); }
.sk-hero.purple { background: linear-gradient(90deg, #41295a 0%, #6947a5 50%, #7c3aed 100%); }
.sk-hero.green  { background: linear-gradient(90deg, #0a5c3d 0%, #16a34a 50%, #22c55e 100%); }
.sk-title { font-size:1.6rem; font-weight:800; margin:0; color:#fff; }
.sk-sub   { opacity:.95; margin-top:8px; font-size:1rem; color:#f1f1f1; }
"""

def sk_inject_css():
    """Call once per page (or once globally) to inject hero CSS."""
    import gradio as gr
    gr.HTML(f"<style>{SMART_HERO_CSS}</style>")

def sk_hero(title: str, subtitle: str, icon: str = "ğŸŒŠ", theme: str = "blue") -> str:
    """Return the hero header HTML. theme: blue | orange | purple | green"""
    theme_class = "" if theme == "blue" else theme
    return f"""
    <div class="sk-hero {theme_class}">
      <p class="sk-title">{icon}&nbsp;&nbsp;{title}</p>
      <p class="sk-sub">{subtitle}</p>
    </div>
    """

def header_bar(title: str, subtitle: str = "") -> str:
    return f"""
    <style>
    .sk-header{{
        background: linear-gradient(90deg,#274060 0%,#3a6ea5 50%,#2d9cdb 100%);
        padding:26px 20px; border-radius:18px; margin-bottom:14px;
        text-align:center; box-shadow:0 4px 12px rgba(0,0,0,.15);
        color:#ffffff !important;                     /* ×œ×‘×Ÿ ×›×‘×¨×™×¨×ª ××—×“×œ */
    }}
    .sk-header .sk-title{{
        color:#ffffff !important;                     /* ××›×¨×™×— ×œ×‘×Ÿ */
        font-size:1.8rem; font-weight:800; margin:0;
        text-shadow:0 1px 1px rgba(0,0,0,.18);
    }}
    .sk-header .sk-sub{{
        color:#ffffff !important;                     /* ××›×¨×™×— ×œ×‘×Ÿ */
        opacity:.95; margin-top:8px; font-size:1rem;
        text-shadow:0 1px 1px rgba(0,0,0,.12);
    }}
    /* ×¨×©×ª ×‘×˜×—×•×Ÿ â€“ ×× ×™×© ×¡×œ×§×˜×•×¨×™× ×ª×•×§×¤× ×™×™×, ×›×œ ×”×¦××¦××™× ×‘×œ×‘×Ÿ */
    .sk-header * {{ color:#ffffff !important; }}
    </style>
    <div class="sk-header">
      <p class="sk-title">{title}</p>
      <p class="sk-sub">{subtitle}</p>
    </div>
    """

# ===== Home Tab (cards UI, with Alerts summary + dark-blue button right next to it) =====
def build_home_tab():
    import gradio as gr

    def _fallback_header_bar(title: str, subtitle: str = "") -> str:
        return f"""
        <div class="app-header">
          <p class="app-title">{title}</p>
          <p class="app-sub">{subtitle}</p>
        </div>
        """

    hb = globals().get("header_bar", _fallback_header_bar)

    # KPIs
    kpis = compute_home_kpis(HOME_TS_LABEL)

    # Alerts count
    n_alerts = len(globals().get("alerts_list", []))
    alerts_txt = ("No active alerts" if n_alerts == 0
                  else f"{n_alerts} active alert{'s' if n_alerts != 1 else ''}")
    dot_class = "dot" if n_alerts == 0 else "dot warn"
    bar_class = "ok" if n_alerts == 0 else "warn"

    status_class = "status" if kpis["is_safe"] else "status advisory"

    header_html = hb(
        title=f"ğŸ“Š Overall Lake Status â€” {HOME_TS_LABEL}",
        subtitle="Real-time monitoring and forecasting system for Lake Kinneret",
    )

    html_top = f"""
    {header_html}

    <!-- Safety strip -->
    <div class="{status_class}"><span class="dot"></span>
      Safety status: <strong>{kpis['safety_status']}</strong>
    </div>

    <!-- KPI cards -->
    <div class="cards">
      <div class="card"><h4>ğŸ’¨ Wind speed</h4><div class="metric">{kpis['wind_speed_str']}</div></div>
      <div class="card"><h4>ğŸŒ¡ï¸ Air temperature</h4><div class="metric">{kpis['air_temp_str']}</div></div>
      <div class="card"><h4>ğŸŒ€ Current intensity</h4><div class="metric">{kpis['current_intensity_str']}</div></div>
      <div class="card"><h4>ğŸŒŠ Wave height</h4><div class="metric">{kpis['wave_hs_str']}</div></div>
      <div class="card"><h4>ğŸ§­ Wind direction</h4><div class="metric">{kpis['wind_dir_str']}</div></div>
      <div class="card"><h4>ğŸ’§ Relative humidity</h4><div class="metric">{kpis['rh_str']}</div></div>
    </div>
    """

    with gr.Group(visible=True) as page_home:
        gr.HTML(html_top)

        # === Yellow alerts pill + dark-blue button, tightly side-by-side ===
        with gr.Row(elem_id="alerts-row"):
            # ×—×©×•×‘: scale=0 ×›×“×™ ×©×œ× ×™×™××ª×—
            with gr.Column(scale=0, min_width=0):
                gr.HTML(f"""
                <div class="alerts-bar {bar_class}">
                  <span class="{dot_class}"></span>
                  <span class="count">{alerts_txt}</span>
                  <span class="hint">Wind â€¢ Currents â€¢ Waves Â· last 2 hours</span>
                </div>
                """)
            with gr.Column(scale=0, min_width=0):
                btn_home_alerts = gr.Button("View alerts", elem_id="btn-view-alerts", elem_classes=["btn-alert"])


    # return the page AND the button so outer code can wire it to _to_alerts
    return page_home, btn_home_alerts

# === CELL 1: prepare station metrics for the map ===
MAP_TS_LABEL = "10/07/2024 10:00"

def get_station_values(target_label=MAP_TS_LABEL):
    """Return dict with metrics for each station at target_label (strings ready for HTML)."""
    data = load_all_data(target_label)
    out = {
        "Ginosar": {"AirTemp":"â€”","RH":"â€”","WindSpeed":"â€”","WindDir":"â€”Â°"},
        "Bteha":   {"AirTemp":"â€”","RH":"â€”","WindSpeed":"â€”","WindDir":"â€”Â°"},
        "EinGev":  {"AirTemp":"â€”","RH":"â€”","WindSpeed":"â€”","WindDir":"â€”Â°"},
        "Zemah":   {"AirTemp":"â€”","RH":"â€”","WindSpeed":"â€”","WindDir":"â€”Â°"},
        "KNW":     {"WaveHs":"â€”","Current":"â€”","CurrentDir":"â€”Â°"},
        "KNC":     {"Current":"â€”","CurrentDir":"â€”Â°"}
    }

    # Ginosar
    if not data["Meto_Ginosar2024.xls"].empty:
        df = data["Meto_Ginosar2024.xls"]
        out["Ginosar"]["AirTemp"] = f"{pd.to_numeric(df['AirTC_Avg'],errors='coerce').mean():.1f} Â°C"
        out["Ginosar"]["RH"] = f"{pd.to_numeric(df['RH'],errors='coerce').mean():.0f} %"
        out["Ginosar"]["WindSpeed"] = f"{pd.to_numeric(df['WS_ms_Avg'],errors='coerce').mean():.2f} m/s"
        out["Ginosar"]["WindDir"] = f"{pd.to_numeric(df['WindDir'],errors='coerce').mean():.0f}Â°"

    # Bteha
    if not data["Meto_Bteha2024.csv"].empty:
        df = data["Meto_Bteha2024.csv"]
        out["Bteha"]["AirTemp"] = "â€”"  # ××™×Ÿ ×¢××•×“×ª ×˜××¤' ×‘×§×•×‘×¥ ×”×–×”
        out["Bteha"]["RH"] = f"{pd.to_numeric(df['Rh'],errors='coerce').mean():.0f} %"
        out["Bteha"]["WindSpeed"] = f"{pd.to_numeric(df['ws_B'],errors='coerce').mean():.2f} m/s"
        out["Bteha"]["WindDir"] = f"{pd.to_numeric(df['Wd'],errors='coerce').mean():.0f}Â°"

    # Ein-Gev
    if not data["Ein_Gev2022-2024.csv"].empty:
        df = data["Ein_Gev2022-2024.csv"]
        out["EinGev"]["AirTemp"] = f"{pd.to_numeric(df['AirTC_Avg'],errors='coerce').mean():.1f} Â°C"
        out["EinGev"]["RH"] = "â€”"  # ××™×Ÿ ×¢××•×“×ª RH ×××•×¦×¢×ª
        out["EinGev"]["WindSpeed"] = f"{pd.to_numeric(df['WS_ms_Avg'],errors='coerce').mean():.2f} m/s"
        out["EinGev"]["WindDir"] = f"{pd.to_numeric(df['WindDir'],errors='coerce').mean():.0f}Â°"

    # Zemah
    if not data["Meto_Zemah2024.csv"].empty:
        df = data["Meto_Zemah2024.csv"]
        out["Zemah"]["AirTemp"] = f"{pd.to_numeric(df['Ta_Z'],errors='coerce').mean():.1f} Â°C"
        out["Zemah"]["RH"] = f"{pd.to_numeric(df['rh_Z'],errors='coerce').mean():.0f} %"
        out["Zemah"]["WindSpeed"] = f"{pd.to_numeric(df['Ws_Z'],errors='coerce').mean():.2f} m/s"
        out["Zemah"]["WindDir"] = f"{pd.to_numeric(df['Wd_Z'],errors='coerce').mean():.0f}Â°"

    # KNW â€“ Waves + Currents
    waves = data["KNW04_Waves.xlsx"]
    if not waves.empty and "Hs (m)" in waves.columns:
        out["KNW"]["WaveHs"] = f"{pd.to_numeric(waves['Hs (m)'],errors='coerce').mean():.2f} m"
    # Current intensity from KNW
    knw = data["KNW04_Currents.csv"]
    if not knw.empty and "Magnitude [cm/s]" in knw.columns:
        mag = pd.to_numeric(knw["Magnitude [cm/s]"],errors="coerce").mean()
        out["KNW"]["Current"] = f"{mag/100.0:.2f} m/s"

    # KNC â€“ Currents only
    knc = data["KNC04_Currents.csv"]
    if not knc.empty and "Magnitude [cm/s]" in knc.columns:
        mag = pd.to_numeric(knc["Magnitude [cm/s]"],errors="coerce").mean()
        out["KNC"]["Current"] = f"{mag/100.0:.2f} m/s"

    return out

MAP_VALUES = get_station_values()

# === CELL 2: build map tab with real values (with blinking dots) â€” unified header + glowing status in modals ===
def build_map_tab():
    import gradio as gr
    import re

    # Get values for the selected timestamp
    vals = get_station_values(MAP_TS_LABEL)

    # --- helpers to compute station status from alerts_list (if available) ---
    def _norm(s: str) -> str:
        return re.sub(r"[^a-z0-9]+", "", str(s).lower())

    def _cat_key(cat):
        # Accept Hebrew/English keys in the data; UI text stays English
        m = {'×¨×•×—': 'wind', '×–×¨××™×': 'currents', '×’×œ×™×': 'waves',
             'wind': 'wind', 'currents': 'currents', 'waves': 'waves'}
        return m.get(str(cat).strip(), 'other')

    def _severity_for(cat_key, v):
        if cat_key == 'wind':     return 'danger' if v >= 3.5 else 'warning'
        if cat_key == 'currents': return 'danger' if v >= 7.0 else 'warning'
        if cat_key == 'waves':    return 'danger' if v >= 0.06 else 'warning'
        return 'warning'

    def station_overall_status(station_name: str) -> str:
        if 'alerts_list' not in globals():
            return 'ok'
        key = _norm(station_name)
        related = [a for a in alerts_list if _norm(a.get('station', '')) == key]
        if not related:
            return 'ok'
        if any(_severity_for(_cat_key(a['category']), float(a['value'])) == 'danger' for a in related):
            return 'danger'
        return 'warning'

    def status_box_html(station_name: str) -> str:
        sev = station_overall_status(station_name)
        when = ALERTS_CONFIG['target'].strftime("%d/%m/%Y %H:%M") if 'ALERTS_CONFIG' in globals() else MAP_TS_LABEL
        if sev == 'danger':
            msg = "â›” Hazardous conditions near this station â€” avoid swimming and small craft."
            klass = "status-danger"
        elif sev == 'warning':
            msg = "âš ï¸ Caution â€” changing conditions near this station."
            klass = "status-warning"
        else:
            msg = "âœ… Safe conditions around this station."
            klass = "status-ok"
        return f'''
          <div class="status-box {klass}">
            <div class="status-main">{msg}</div>
            <div class="status-sub">As of {when}</div>
          </div>
        '''

    css = """
    .map-instruction{
      background:linear-gradient(135deg,#e8f4fd 0%,#f0f8ff 100%);
      border:2px solid #2196f3; border-radius:10px; padding:12px; text-align:center; margin-bottom:12px;
    }
    .map-instruction b{color:#1565c0}

    .map-wrap{ position:relative; background:#fff; border:1px solid #e6eef6; border-radius:16px;
      box-shadow:0 6px 16px rgba(31,41,55,.08); padding:16px;}
    .map-svg{ width:100%; height:520px; background:linear-gradient(135deg,#e3f2fd 0%,#bbdefb 100%);
      border-radius:12px;}
    :root { --water:#5bb4ff; --shore:#0a3a66; }
    .lake{ fill:var(--water); stroke:var(--shore); stroke-width:6; }

    /* Base dot styles */
    .station-dot{
      stroke:#fff; stroke-width:3; cursor:pointer;
      transform-box: fill-box;
      transform-origin: 50% 50%;
    }
    .station-land{ fill:#4caf50; }
    .station-water{ fill:#2196f3; }

    /* Blink animation */
    .station-dot.blink { animation: blink 1.2s infinite; }
    @keyframes blink { 0%,49%{opacity:1} 50%,100%{opacity:.35} }
    @media (prefers-reduced-motion: reduce) { .station-dot.blink{ animation:none } }

    .legend{ position:absolute; left:16px; bottom:16px; background:rgba(255,255,255,.95);
      border:1px solid #e6eef6; border-radius:10px; padding:12px; box-shadow:0 4px 12px rgba(0,0,0,.06); }
    .legend .row{ display:flex; align-items:center; gap:8px; margin:6px 0;}
    .badge{ width:14px; height:14px; border-radius:50%; border:2px solid #fff; }
    .badge.land{ background:#4caf50; } .badge.water{ background:#2196f3; }

    .grid{ display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:12px; }
    .kv{ background:#f8fafe; border:1px solid #e6eef6; border-radius:10px; padding:12px; }
    .kv .k{ font-size:.76rem; color:#6b7a90; text-transform:uppercase; letter-spacing:.03em; margin-bottom:6px;}
    .kv .v{ font-size:1.05rem; font-weight:700; color:#2a3b4c;}

    /* CSS-only modal via :target */
    .map-modal{ position:fixed; inset:0; display:none; align-items:center; justify-content:center;
      background:rgba(0,0,0,.4); z-index:9999; }
    .map-modal:target{ display:flex; }
    .map-modal-content{
      width:min(880px,90vw); max-height:85vh; overflow:auto;
      background:#fff; border-radius:16px; padding:18px;
      box-shadow:0 12px 32px rgba(0,0,0,.18); border:1px solid #e6eef6;
    }
    .map-modal-header{ display:flex; align-items:center; justify-content:space-between; margin-bottom:10px;}
    .map-modal-title{ margin:0; font-size:1.15rem; font-weight:800; color:#2a3b4c;}
    .map-modal-close{
      display:inline-flex; align-items:center; justify-content:center; text-decoration:none;
      background:#f3f6fb; border:1px solid #e1e8f5; width:34px; height:34px; border-radius:8px;
      font-size:18px; color:#334; line-height:0;
    }
    .map-modal-close:hover{ background:#eaf0fb; }
    .modal-footer{ margin-top:10px; font-size:.85rem; color:#6b7a90; }
    svg a { cursor: pointer; }

    /* Glowing station-status box */
    .status-box{ margin:4px 0 14px 0; padding:12px 14px; border-radius:12px; font-weight:800;
                 border:2px solid; line-height:1.3; }
    .status-main{ font-size:15px; }
    .status-sub{ font-size:12px; opacity:.75; margin-top:4px; }
    .status-ok{ color:#1b5e20; background:linear-gradient(180deg,#e9f7ef,#e1f5e9);
                border-color:#43a047; box-shadow:0 0 0 3px rgba(67,160,71,.2), 0 0 18px rgba(67,160,71,.35); }
    .status-warning{ color:#7a4d00; background:linear-gradient(180deg,#fff7e6,#fff3cd);
                     border-color:#ff9800; box-shadow:0 0 0 3px rgba(255,152,0,.25), 0 0 18px rgba(255,152,0,.45); }
    .status-danger{ color:#7f1d1d; background:linear-gradient(180deg,#fdecea,#fde0dc);
                    border-color:#e53935; box-shadow:0 0 0 3px rgba(229,57,53,.25), 0 0 20px rgba(229,57,53,.55); }
    """

    html = f"""
    <div class="map-instruction">ğŸ‘‰ <b>Click on a station</b> to open its popup with real values.</div>

    <div class="map-wrap">
      <svg class="map-svg" viewBox="0 0 800 600" preserveAspectRatio="xMidYMid meet">
        <!-- Lake -->
        <g transform="translate(90,12) scale(0.56)">
          <path class="lake" d="M628.21,0.00 L737.18,89.74 L820.51,205.13 L807.69,326.92 L826.92,352.56 L826.92,743.59 L679.49,1000.00 L570.51,1000.00 L455.13,743.59 L358.97,660.26 L314.10,538.46 L237.18,487.18 L173.08,403.85 L192.31,301.28 L262.82,153.85 L557.69,0.00 Z"/>
        </g>

        <!-- Land meteo stations -->
        <a href="#modal-ginosar">
          <circle class="station-dot station-land blink" cx="210" cy="260" r="15"/>
          <text x="210" y="240" text-anchor="middle" font-size="12" font-weight="bold">Ginosar</text>
        </a>

        <a href="#modal-bteha">
          <circle class="station-dot station-land blink" cx="540" cy="140" r="15"/>
          <text x="540" y="120" text-anchor="middle" font-size="12" font-weight="bold">Bteha</text>
        </a>

        <a href="#modal-eingev">
          <circle class="station-dot station-land blink" cx="520" cy="400" r="15"/>
          <text x="520" y="420" text-anchor="middle" font-size="12" font-weight="bold">Ein-Gev</text>
        </a>

        <a href="#modal-zemah">
          <circle class="station-dot station-land blink" cx="385" cy="525" r="15"/>
          <text x="385" y="545" text-anchor="middle" font-size="12" font-weight="bold">Zemah</text>
        </a>

        <!-- Water stations -->
        <a href="#modal-knw">
          <circle class="station-dot station-water blink" cx="480" cy="270" r="15"/>
          <text x="480" y="238" text-anchor="middle" font-size="12" font-weight="bold" fill="white">KNW</text>
        </a>

        <a href="#modal-knc">
          <circle class="station-dot station-water blink" cx="480" cy="370" r="15"/>
          <text x="480" y="338" text-anchor="middle" font-size="12" font-weight="bold" fill="white">KNC</text>
        </a>
      </svg>

      <div class="legend">
        <div style="font-weight:700; margin-bottom:6px;">Legend</div>
        <div class="row"><span class="badge land"></span><span>Land (Meteo) station</span></div>
        <div class="row"><span class="badge water"></span><span>Lake (Water) station</span></div>
        <div style="font-size:12px;color:#6b7a90;margin-top:6px; line-height:1.4">
          â€¢ KNW = Golan Beach (Waves + Currents)<br/>â€¢ KNC = F Station (Currents only)
        </div>
      </div>
    </div>

    <!-- ===== POPUPS (one per station) ===== -->
    <!-- Land: Ginosar -->
    <div id="modal-ginosar" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-ginosar">
        <div class="map-modal-header">
          <h3 id="ttl-ginosar" class="map-modal-title">Ginosar (Meteo)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('Ginosar')}
        <div class="grid">
          <div class="kv"><div class="k">Air temperature</div><div class="v">{vals['Ginosar']['AirTemp']}</div></div>
          <div class="kv"><div class="k">Relative humidity</div><div class="v">{vals['Ginosar']['RH']}</div></div>
          <div class="kv"><div class="k">Wind speed</div><div class="v">{vals['Ginosar']['WindSpeed']}</div></div>
          <div class="kv"><div class="k">Wind direction</div><div class="v">{vals['Ginosar']['WindDir']}</div></div>
          <div class="kv"><div class="k">Measurement freq.</div><div class="v">Every 10 min</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Meteo (air) variables</div></div>
        </div>
      </div>
    </div>

    <!-- Land: Bteha -->
    <div id="modal-bteha" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-bteha">
        <div class="map-modal-header">
          <h3 id="ttl-bteha" class="map-modal-title">Bteha (Meteo)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('Bteha')}
        <div class="grid">
          <div class="kv"><div class="k">Air temperature</div><div class="v">{vals['Bteha'].get('AirTemp','â€”')}</div></div>
          <div class="kv"><div class="k">Relative humidity</div><div class="v">{vals['Bteha']['RH']}</div></div>
          <div class="kv"><div class="k">Wind speed</div><div class="v">{vals['Bteha']['WindSpeed']}</div></div>
          <div class="kv"><div class="k">Wind direction</div><div class="v">{vals['Bteha']['WindDir']}</div></div>
          <div class="kv"><div class="k">Measurement freq.</div><div class="v">Every 10 min</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Meteo (air) variables</div></div>
        </div>
      </div>
    </div>

    <!-- Land: Ein-Gev -->
    <div id="modal-eingev" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-eingev">
        <div class="map-modal-header">
          <h3 id="ttl-eingev" class="map-modal-title">Ein-Gev (Meteo)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('Ein-Gev')}
        <div class="grid">
          <div class="kv"><div class="k">Air temperature</div><div class="v">{vals['EinGev']['AirTemp']}</div></div>
          <div class="kv"><div class="k">Relative humidity</div><div class="v">{vals['EinGev']['RH']}</div></div>
          <div class="kv"><div class="k">Wind speed</div><div class="v">{vals['EinGev']['WindSpeed']}</div></div>
          <div class="kv"><div class="k">Wind direction</div><div class="v">{vals['EinGev']['WindDir']}</div></div>
          <div class="kv"><div class="k">Measurement freq.</div><div class="v">Every 10 min</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Meteo (air) variables</div></div>
        </div>
      </div>
    </div>

    <!-- Land: Zemah -->
    <div id="modal-zemah" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-zemah">
        <div class="map-modal-header">
          <h3 id="ttl-zemah" class="map-modal-title">Zemah (Meteo)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('Zemah')}
        <div class="grid">
          <div class="kv"><div class="k">Air temperature</div><div class="v">{vals['Zemah']['AirTemp']}</div></div>
          <div class="kv"><div class="k">Relative humidity</div><div class="v">{vals['Zemah']['RH']}</div></div>
          <div class="kv"><div class="k">Wind speed</div><div class="v">{vals['Zemah']['WindSpeed']}</div></div>
          <div class="kv"><div class="k">Wind direction</div><div class="v">{vals['Zemah']['WindDir']}</div></div>
          <div class="kv"><div class="k">Measurement freq.</div><div class="v">Every 10 min</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Meteo (air) variables</div></div>
        </div>
      </div>
    </div>

    <!-- Water: KNW -->
    <div id="modal-knw" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-knw">
        <div class="map-modal-header">
          <h3 id="ttl-knw" class="map-modal-title">KNW â€“ Golan Beach (Waves + Currents)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('KNW')}
        <div class="grid">
          <div class="kv"><div class="k">Wave height</div><div class="v">{vals['KNW']['WaveHs']}</div></div>
          <div class="kv"><div class="k">Current intensity</div><div class="v">{vals['KNW']['Current']}</div></div>
          <div class="kv"><div class="k">Current direction</div><div class="v">{vals['KNW'].get('CurrentDir','â€”Â°')}</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Waves + Currents</div></div>
        </div>
      </div>
    </div>

    <!-- Water: KNC -->
    <div id="modal-knc" class="map-modal" aria-hidden="true">
      <div class="map-modal-content" role="dialog" aria-modal="true" aria-labelledby="ttl-knc">
        <div class="map-modal-header">
          <h3 id="ttl-knc" class="map-modal-title">KNC â€“ F Station (Currents only)</h3>
          <a href="#" class="map-modal-close" aria-label="Close">Ã—</a>
        </div>
        {status_box_html('KNC')}
        <div class="grid">
          <div class="kv"><div class="k">Current intensity</div><div class="v">{vals['KNC']['Current']}</div></div>
          <div class="kv"><div class="k">Current direction</div><div class="v">{vals['KNC'].get('CurrentDir','â€”Â°')}</div></div>
          <div class="kv"><div class="k">Depth layer</div><div class="v">0â€“5 m</div></div>
          <div class="kv"><div class="k">Trend (10 min)</div><div class="v">â€”</div></div>
          <div class="kv"><div class="k">Capabilities</div><div class="v">Currents only</div></div>
        </div>
        <div class="modal-footer">â„¹ï¸ This station does not measure waves.</div>
      </div>
    </div>
    """

    with gr.Group(visible=False) as page_map:
        # Unified header in the same style as All/Specific Simulations
        gr.HTML(header_bar(
            title="ğŸ—ºï¸ Stations Map â€” Monitoring",
            subtitle=f"Interactive map of Lake Kinneret stations â€¢ click a station to see details for {MAP_TS_LABEL}"
        ))
        gr.HTML("<style>"+css+"</style>"+html)  # no sanitize
    return page_map

import pandas as pd
from urllib.parse import quote

# ---------- Repo ----------
USER, REPO, BRANCH = "NahlaAboromi", "ecological-modeling-lab-team-biodynamics", "main"
def raw_url(path: str) -> str:
    return f"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{quote(path)}"

# ---------- Files & datetime columns ----------
FILES = {
    # Meteo (land)
    "Ein_Gev2022-2024.csv": {"path": "Ein_Gev2022-2024.csv", "type": "csv",  "dt_col": ("DateTime",)},
    "Meto_Bteha2024.csv":   {"path": "Meto_Bteha2024.csv",   "type": "csv",  "dt_col": ("DateTime",)},
    "Meto_Ginosar2024.xls": {"path": "Meto_Ginosar2024.xls", "type": "xls",  "dt_col": ("DateTime","date","time")},
    "Meto_Zemah2024.csv":   {"path": "Meto_Zemah2024.csv",   "type": "csv",  "dt_col": ("DateTime",)},
    # Lake (water)
    "KNC04_Currents.csv":   {"path": "KNC04_Currents.csv",   "type": "csv",  "dt_col": ("Date",)},
    "KNW04_Currents.csv":   {"path": "KNW04_Currents.csv",   "type": "csv",  "dt_col": ("Date",)},
    "KNW04_Waves.xlsx":     {"path": "KNW04_Waves.xlsx",     "type": "xlsx", "dt_col": ("Date & Time (GMT)",)},  # ×©×™××™ ×œ×‘: GMT
}

# ---------- Target instant (dd/mm/yyyy HH:MM) ----------
TARGET = pd.to_datetime("10/07/2024 10:00", dayfirst=True)

# ---------- IO helpers ----------
def read_any(url: str, ftype: str) -> pd.DataFrame:
    if ftype == "csv":
        return pd.read_csv(url)
    if ftype == "xls":
        return pd.read_excel(url, engine="xlrd")
    if ftype == "xlsx":
        return pd.read_excel(url, engine="openpyxl")
    raise ValueError(f"Unknown file type: {ftype}")

def ensure_when(df: pd.DataFrame, dt_cols: tuple) -> pd.Series:
    """
    ××™×™×¦×¨ ×¢××•×“×ª datetime ××—×™×“×” ×‘×©× _when (××™×•×©×¨×ª ×œ×“×§×”), ×œ×¤×™ ×§×“×™××•×™×•×ª:
      1) ×× ×™×© 'DateTime' â†’ ×œ×¤×¨×© ×¢× dayfirst=True
      2) ×× ×™×© ×¢××•×“×ª datetime ×‘×•×“×“×ª (×œ× 'date'/'time') â†’ ×œ×¤×¨×©
      3) ×× ×™×© ('date','time') â†’ ×œ×©×œ×‘ ×•×œ×¤×¨×©
      4) fallback: ×”×¢××•×“×” ×”×¨××©×•× ×” ×‘×¨×©×™××ª dt_cols ×©×§×™×™××ª
    """
    # 1) 'DateTime'
    if "DateTime" in df.columns:
        return pd.to_datetime(df["DateTime"].astype(str), dayfirst=True, errors="coerce")

    # 2) Single datetime column (excluding 'date'/'time')
    for c in dt_cols:
        if c in df.columns and c.lower() not in ("date", "time"):
            return pd.to_datetime(df[c].astype(str), dayfirst=True, errors="coerce")

    # 3) Combine date + time (×‘×›×œ ×›×ª×™×‘)
    date_col = next((c for c in ("date","Date","DATE") if c in df.columns), None)
    time_col = next((c for c in ("time","Time","TIME") if c in df.columns), None)
    if date_col and time_col:
        combo = (df[date_col].astype(str).str.strip() + " " + df[time_col].astype(str).str.strip())
        return pd.to_datetime(combo, dayfirst=True, errors="coerce")

    # 4) fallback
    for c in dt_cols:
        if c in df.columns:
            return pd.to_datetime(df[c].astype(str), dayfirst=True, errors="coerce")

    raise KeyError("No datetime columns found to parse")

# ---------- Build the 100-prev tables ----------
prev100_results = {}

for fname, cfg in FILES.items():
    df = read_any(raw_url(cfg["path"]), cfg["type"])
    when = ensure_when(df, cfg["dt_col"]).dt.floor("min")
    df = df.copy()
    df["_when"] = when
    df = df[df["_when"].notna()]
    prev = df[df["_when"] < TARGET].sort_values("_when").tail(100).reset_index(drop=True)
    prev100_results[fname] = prev

# ---------- FULL display settings (no "...") ----------
pd.options.display.max_rows = None
pd.options.display.max_columns = None
pd.options.display.width = 0              # ××œ ×ª×©×‘×•×¨ ×©×•×¨×•×ª
pd.options.display.max_colwidth = None
pd.options.display.float_format = "{:.3f}".format

# ---------- Print all tables in full ----------
print("=== Last 100 rows BEFORE 10/07/2024 10:00 for each file (FULL DISPLAY) ===")
for fname, prev in prev100_results.items():
    print(f"\nâ€¢ {fname}: {len(prev)} rows")
    if not prev.empty:
        print(f"  Range: {prev['_when'].iloc[0]} â†’ {prev['_when'].iloc[-1]}")
    else:
        print("  (no rows found before target)")
    display(prev)

# ×˜×™×¤: ×× ×ª×¨×¦×™ ×œ×”×—×–×™×¨ ×œ×‘×¨×™×¨×•×ª ××—×“×œ, ××¤×©×¨:
# pd.reset_option("display.max_rows"); pd.reset_option("display.max_columns")
# pd.reset_option("display.width"); pd.reset_option("display.max_colwidth"); pd.reset_option("display.float_format")

# === CELL: alert functions (EN only) â€” wind / currents / waves ===
import pandas as pd
import numpy as np

# Default config
ALERTS_CONFIG = {
    "target":        pd.to_datetime("10/07/2024 10:00", dayfirst=True),
    "window_min":    120,   # minutes window before the latest timestamp in each file
    "wind_thresh":   2.2,   # m/s   strong wind
    "curr_thresh":   4.5,   # cm/s  strong currents
    "waves_dths":    0.01,  # m     |Î”Hs| in the last hour
    "waves_abs":     0.04,  # m     absolute Hs threshold
    "max_alerts":    3,     # return up to N alerts
    "one_per_type":  False, # if True: pick at most one of each type (wind/currents/waves)
}

def _last_window(df: pd.DataFrame, minutes: int) -> pd.DataFrame:
    """Return the last `minutes` window by _when."""
    if df is None or df.empty or "_when" not in df.columns:
        return pd.DataFrame()
    end = df["_when"].max()
    start = end - pd.to_timedelta(minutes, unit="min")
    return df[(df["_when"] >= start) & (df["_when"] <= end)].copy()

def _closest_to_target(ts: pd.Timestamp, target: pd.Timestamp) -> float:
    """Distance in seconds to the target time (for sorting by closeness)."""
    return abs((target - ts).total_seconds())

def _numeric(s):
    return pd.to_numeric(s, errors="coerce")

def compute_alerts(prev100_results: dict, config: dict | None = None, *, verbose: bool = False):
    """
    Returns (alerts_df, alerts_list).
      - alerts_df: summary table ready for UI (English columns).
      - alerts_list: list of dicts per alert, each includes 'support' DataFrame.
    Expects `prev100_results` from the previous cell: {filename: DataFrame with _when}.
    """
    if config is None:
        config = ALERTS_CONFIG

    TARGET      = config.get("target")
    WINDOW_MIN  = int(config.get("window_min", 120))
    WIND_THRESH = float(config.get("wind_thresh", 2.2))
    CURR_THRESH = float(config.get("curr_thresh", 4.5))
    WAVES_DTHS  = float(config.get("waves_dths", 0.01))
    WAVES_ABS   = float(config.get("waves_abs", 0.04))
    MAX_ALERTS  = int(config.get("max_alerts", 3))
    ONE_PER     = bool(config.get("one_per_type", False))

    alerts = []

    # ---- WIND (meteo stations) ----
    wind_specs = {
        "Ein_Gev2022-2024.csv": {"station": "Ein Gev", "speed_col": "WS_ms_Avg", "dir_col": "WindDir"},
        "Meto_Bteha2024.csv":   {"station": "Bteha",   "speed_col": "ws_B",      "dir_col": "Wd"},
        "Meto_Zemah2024.csv":   {"station": "Zemah",   "speed_col": "Ws_Z",      "dir_col": "Wd_Z"},
        "Meto_Ginosar2024.xls": {"station": "Ginosar", "speed_col": "WS_ms_Avg", "dir_col": "WindDir"},
    }
    for fname, spec in wind_specs.items():
        df = prev100_results.get(fname)
        if df is None or df.empty or spec["speed_col"] not in df.columns:
            continue
        w = _last_window(df, WINDOW_MIN)
        if w.empty:
            continue
        w["speed"] = _numeric(w[spec["speed_col"]])
        w["dir"]   = _numeric(w.get(spec["dir_col"], np.nan))
        w = w[w["speed"].notna()]
        if w.empty:
            continue
        peak = w.loc[w["speed"].idxmax()]
        if float(peak["speed"]) >= WIND_THRESH:
            alerts.append({
                "category": "wind",
                "title": "Strong winds near station",
                "station": spec["station"],
                "when": peak["_when"],
                "detail": f"Wind speed {peak['speed']:.1f} m/s at {spec['station']}"
                          + (f", direction {int(round(peak['dir']))}Â°" if pd.notna(peak["dir"]) else ""),
                "value": float(peak["speed"]),
                "unit": "m/s",
                "support": w.sort_values("_when"),
            })

    # ---- CURRENTS (KNC/KNW) ----
    curr_specs = {
        "KNC04_Currents.csv": {"station": "KNC (F)"},
        "KNW04_Currents.csv": {"station": "KNW (Golan Beach)"},
    }
    for fname, spec in curr_specs.items():
        df = prev100_results.get(fname)
        if df is None or df.empty:
            continue
        w = _last_window(df, WINDOW_MIN)
        if w.empty:
            continue
        for c in ["Depth", "Magnitude [cm/s]"]:
            if c in w.columns:
                w[c] = _numeric(w[c])
        candidate = w[w["Depth"].between(2, 5)] if "Depth" in w.columns else w
        if candidate.empty:
            candidate = w
        if "Magnitude [cm/s]" not in candidate.columns or candidate["Magnitude [cm/s]"].dropna().empty:
            continue
        peak = candidate.loc[candidate["Magnitude [cm/s]"].idxmax()]
        if float(peak["Magnitude [cm/s]"]) >= CURR_THRESH:
            alerts.append({
                "category": "currents",
                "title": "Increased current intensity",
                "station": spec["station"],
                "when": peak["_when"],
                "detail": f"Currents at {int(peak['Depth']) if 'Depth' in candidate.columns and pd.notna(peak['Depth']) else '?'} m: "
                          f"{peak['Magnitude [cm/s]']:.2f} cm/s at {spec['station']}",
                "value": float(peak["Magnitude [cm/s]"]),
                "unit": "cm/s",
                "support": candidate.sort_values(["_when", "Depth"]) if "Depth" in candidate.columns else candidate.sort_values("_when"),
            })

    # ---- WAVES (KNW waves) ----
    waves_name = "KNW04_Waves.xlsx"
    wv = prev100_results.get(waves_name)
    if wv is not None and not wv.empty and "Hs (m)" in wv.columns:
        w = _last_window(wv.copy(), WINDOW_MIN)
        if not w.empty:
            w["Hs (m)"] = _numeric(w["Hs (m)"])
            w = w[w["Hs (m)"].notna()].sort_values("_when")
            if len(w) >= 2:
                last = w.iloc[-1]; prev = w.iloc[-2]
                delta = float(last["Hs (m)"] - prev["Hs (m)"])
                if abs(delta) >= WAVES_DTHS or float(last["Hs (m)"]) >= WAVES_ABS:
                    alerts.append({
                        "category": "waves",
                        "title": "Sudden change in wave height",
                        "station": "KNW (Golan Beach)",
                        "when": last["_when"],
                        "detail": f"Hs={last['Hs (m)']:.2f} m (Î” {delta:+.02f} m in the last hour)",
                        "value": float(last["Hs (m)"]),
                        "unit": "m",
                        "support": w,
                    })

    # ---- Final selection: closest to TARGET (optionally one per type) ----
    if ONE_PER:
        chosen = []
        for cat in ("wind", "currents", "waves"):
            cands = [a for a in alerts if a["category"] == cat]
            if cands:
                cands = sorted(cands, key=lambda a: _closest_to_target(a["when"], TARGET))
                chosen.append(cands[0])
        if len(chosen) < MAX_ALERTS:
            remaining = [a for a in sorted(alerts, key=lambda a: _closest_to_target(a["when"], TARGET))
                         if a not in chosen]
            chosen += remaining[:MAX_ALERTS - len(chosen)]
        final_alerts = chosen[:MAX_ALERTS]
    else:
        final_alerts = sorted(alerts, key=lambda a: _closest_to_target(a["when"], TARGET))[:MAX_ALERTS]

    # ---- UI summary table (English columns) ----
    rows = []
    for a in final_alerts:
        mins_before = int((TARGET - a["when"]).total_seconds() // 60)
        rows.append({
            "Type": a["category"],
            "Title": a["title"],
            "Station": a["station"],
            "Time": a["when"],
            "Minutes before target": mins_before,
            "Value": a["value"],
            "Unit": a["unit"],
            "Description": a["detail"],
        })
    alerts_df = pd.DataFrame(rows).sort_values("Time") if rows else pd.DataFrame(
        columns=["Type","Title","Station","Time","Minutes before target","Value","Unit","Description"]
    )
    return alerts_df, final_alerts

# === NEXT CELL: run & display ===
# Use defaults, or override (e.g., {"one_per_type": True})
cfg = ALERTS_CONFIG | {"one_per_type": False}
alerts_df, alerts_list = compute_alerts(prev100_results, config=cfg)

print("=== Alerts (wind/currents/waves) â€” up to 3 closest to target ===")
display(alerts_df)  # Expected columns: Type, Title, Station, Time, Minutes before target, Value, Unit, Description

# Optional: show supporting window (last 10 rows) for each alert
for i, a in enumerate(alerts_list, 1):
    print(f"\nSupport table for alert #{i} â€” {a['title']} ({a['category']} @ {a['station']})")
    display(a["support"].tail(10))

# === ALERTS PAGE (Details modal includes glowing station-status box, ENGLISH UI) ===
import pandas as pd, json
from html import escape

assert 'alerts_list' in globals() and 'alerts_df' in globals(), \
    "Run the alerts computation cell first to create alerts_list / alerts_df."

TARGET = ALERTS_CONFIG['target']

def _cat_key(cat):
    # Accept Hebrew or English category labels in the data; UI stays English
    m = {'×¨×•×—':'wind','×–×¨××™×':'currents','×’×œ×™×':'waves',
         'wind':'wind','currents':'currents','waves':'waves'}
    return m.get(str(cat).strip(), 'other')

def _title_for(cat_key):
    return {'wind':'Strong winds near station',
            'currents':'Increased current intensity',
            'waves':'Sudden change in wave height'}.get(cat_key, 'Alert')

def _severity_for(cat_key, v):
    if cat_key == 'wind':     return 'danger' if v >= 3.5 else 'warning'
    if cat_key == 'currents': return 'danger' if v >= 7.0 else 'warning'
    if cat_key == 'waves':    return 'danger' if v >= 0.06 else 'warning'
    return 'warning'

def _desc_for(a, cat_key):
    station, value = a['station'], float(a['value'])
    supp = a['support']
    if cat_key == 'wind':
        direction = None
        if {'speed','dir'}.issubset(supp.columns) and not supp['speed'].dropna().empty:
            r = supp.loc[supp['speed'].idxmax()]
            try: direction = int(round(float(r['dir'])))
            except Exception: direction = None
        return f"Wind speed {value:.1f} m/s at {station}" + (f", direction {direction}Â°" if direction is not None else "")
    elif cat_key == 'currents':
        depth = None
        if 'Magnitude [cm/s]' in supp.columns and not supp['Magnitude [cm/s]'].dropna().empty:
            r = supp.loc[supp['Magnitude [cm/s]'].idxmax()]
            if 'Depth' in r and pd.notna(r['Depth']):
                try: depth = int(r['Depth'])
                except Exception: depth = None
        return f"Currents at {depth if depth is not None else '?'} m: {value:.2f} cm/s at {station}"
    else:
        w = supp.sort_values('_when')
        delta = None
        if 'Hs (m)' in w.columns and len(w) >= 2:
            delta = float(w['Hs (m)'].iloc[-1] - w['Hs (m)'].iloc[-2])
        return f"Hs={value:.2f} m at {station}" + (f" (Î” {delta:+.02f} m in last hour)" if delta is not None else "")

def _explain_for(a, cat_key):
    station = a['station']
    if cat_key == 'wind':
        cause = f"Recent wind measurements at {station} exceeded the configured threshold ({ALERTS_CONFIG['wind_thresh']} m/s)."
        impact = "Higher waves likely; small craft handling becomes difficult near shore."
        rec = "Beginners: avoid going out. Experienced skippers: use full safety gear and stay close to shore."
    elif cat_key == 'currents':
        s = a['support']; depth = None
        if 'Depth' in s.columns and not s['Depth'].dropna().empty:
            try: depth = int(s.loc[s['Magnitude [cm/s]'].idxmax()]['Depth'])
            except Exception: depth = None
        cause = f"Strengthening currents detected in the {'upper layer' if depth is None else f'~{depth} m layer'} at {station}."
        impact = "Swimming against the current becomes difficult; risk of being pulled away from the beach."
        rec = "Swim parallel to shore and supervise children; avoid small-craft boating in the affected area."
    else:
        cause = f"Increase in significant wave height (Hs) at {station} above monitoring thresholds."
        impact = "High waves complicate swimming and boating; risk near the waterline."
        rec = "Avoid small craft; swimmers should wait until waves subside."
    return cause, impact, rec

# ---- NEW: overall station status (ok / warning / danger) ----
def _station_overall_severity(station: str) -> str:
    s = str(station).strip().lower()
    rel = [a for a in alerts_list if str(a.get('station','')).strip().lower() == s]
    if not rel:
        return 'ok'
    if any(_severity_for(_cat_key(a['category']), float(a['value'])) == 'danger' for a in rel):
        return 'danger'
    return 'warning'

def _status_box_html(station: str) -> str:
    sev = _station_overall_severity(station)
    if sev == 'danger':
        msg = "â›” Hazardous conditions near this station â€” avoid swimming and small craft."
        klass = "status-danger"
    elif sev == 'warning':
        msg = "âš ï¸ Caution â€” changing conditions near this station."
        klass = "status-warning"
    else:
        msg = "âœ… Safe conditions around this station."
        klass = "status-ok"
    when = TARGET.strftime("%d/%m/%Y %H:%M")
    return f'''
      <div class="status-box {klass}">
        <div class="status-main">{escape(msg)}</div>
        <div class="status-sub">As of {escape(when)}</div>
      </div>
    '''

cards_html, modals_html = [], []

for i, a in enumerate(alerts_list, 1):
    aid      = f"alert{i}"
    cat_key  = _cat_key(a['category'])
    title    = _title_for(cat_key)
    desc     = _desc_for(a, cat_key)
    sev      = _severity_for(cat_key, float(a['value']))
    mins     = int((TARGET - a['when']).total_seconds() // 60)
    time_txt = f"â‰ˆ {mins} min before target | last update: {a['when']:%Y-%m-%d %H:%M:%S}"
    cause, impact, rec = _explain_for(a, cat_key)

    # card
    cards_html.append(f"""
    <div class="alert-item {'danger' if sev=='danger' else 'warning'}" id="{aid}">
      <div class="alert-inner">
        <div class="alert-content">
          <div class="alert-title">{escape(title)}</div>
          <div class="alert-description">{escape(desc)}</div>
          <div class="alert-time">{escape(time_txt)}</div>
        </div>
        <div class="alert-actions">
          <label class="btn btn-primary" for="modal-{aid}">Details</label>
        </div>
      </div>
    </div>
    """)

    # modal (checkbox toggle hidden via CSS) + NEW status box at top
    modals_html.append(f"""
    <input type="checkbox" id="modal-{aid}" class="modal-toggle">
    <div class="popup">
      <label class="popup-overlay" for="modal-{aid}"></label>
      <div class="popup-content">
        <div class="popup-header">
          <div class="popup-title">Alert details â€” {escape(title)}</div>
          <label class="close-btn" for="modal-{aid}">Ã—</label>
        </div>

        {_status_box_html(a['station'])}

        <div class="popup-section">
          <h4 class="section-title">Cause</h4>
          <p>{escape(cause)}</p>
        </div>
        <div class="popup-section">
          <h4 class="section-title">Expected impact</h4>
          <p>{escape(impact)}</p>
        </div>
        <div class="popup-section">
          <h4 class="section-title">Recommendations</h4>
          <p>{escape(rec)}</p>
        </div>
      </div>
    </div>
    """)

ALERTS_HTML = f"""
<div id="alerts-page">
  <style>
    .modal-toggle {{
      position:absolute !important; left:-9999px !important;
      width:0 !important; height:0 !important; opacity:0 !important; pointer-events:none !important;
    }}

    .alert-list {{ display:flex; flex-direction:column; gap:16px; }}
    .alert-item {{ background:#fff; border-radius:12px; padding:0;
                   box-shadow:0 4px 15px rgba(0,0,0,.08);
                   border-right:5px solid #f44336; transition:transform .2s, box-shadow .2s; }}
    .alert-item.warning {{ border-right-color:#ff9800; }}
    .alert-item:hover {{ transform: translateX(-4px); box-shadow:0 8px 24px rgba(0,0,0,.12); }}
    .alert-inner {{ display:flex; justify-content:space-between; align-items:center; padding:20px; gap:16px; }}
    .alert-content {{ flex:1; min-width:0; }}
    .alert-title {{ font-size:16px; font-weight:700; color:#333; margin-bottom:8px; }}
    .alert-description {{ font-size:14px; color:#666; margin-bottom:6px; }}
    .alert-time {{ font-size:12px; color:#999; }}
    .alert-actions {{ display:flex; gap:10px; }}

    .btn {{ padding:8px 14px; border-radius:6px; font-size:14px; font-weight:700; cursor:pointer;
           user-select:none; border:1px solid transparent; }}
    .btn-primary {{ background:#1e88e5; border-color:#1976d2; color:#fff; }}
    .btn-primary:hover {{ background:#1976d2; }}

    .modal-toggle + .popup {{ display:none; }}
    .modal-toggle:checked + .popup {{ display:flex; }}
    .popup {{ position:fixed; inset:0; background:rgba(0,0,0,.5); justify-content:center; align-items:center; z-index:9999; }}
    .popup-overlay {{ position:absolute; inset:0; }}
    .popup-content {{ position:relative; background:#fff; border-radius:14px; padding:26px;
                      width:min(92vw,560px); max-height:82vh; overflow:auto;
                      box-shadow:0 20px 40px rgba(0,0,0,.25); }}
    .popup-header {{ display:flex; justify-content:space-between; align-items:center;
                     border-bottom:2px solid #eee; padding-bottom:10px; margin-bottom:14px; }}
    .popup-title {{ font-size:20px; font-weight:700; color:#333; }}
    .close-btn {{ font-size:22px; color:#999; cursor:pointer; padding:2px 8px; border-radius:6px; border:1px solid transparent; }}
    .close-btn:hover {{ background:#f5f5f5; color:#666; border-color:#e0e0e0; }}
    .section-title {{ color:#e53935; margin:10px 0 6px; font-weight:700; }}
    .popup-section p {{ color:#555; line-height:1.6; }}

    /* Header */
    .alerts-header {{ background:#e9e9ee; border-radius:8px; padding:12px 14px; margin-bottom:12px;
                      border-left:6px solid #f0b429; display:flex; align-items:center; gap:8px; }}
    .alerts-header h3 {{ margin:0; font-size:18px; }}

    /* NEW: glowing station-status box */
    .status-box {{
      margin:8px 0 14px 0; padding:12px 14px; border-radius:12px; font-weight:800;
      border:2px solid; line-height:1.3;
    }}
    .status-main {{ font-size:15px; }}
    .status-sub  {{ font-size:12px; opacity:.75; margin-top:4px; }}

    .status-ok {{
      color:#1b5e20; background:linear-gradient(180deg,#e9f7ef,#e1f5e9);
      border-color:#43a047; box-shadow:0 0 0 3px rgba(67,160,71,.2), 0 0 18px rgba(67,160,71,.35);
    }}
    .status-warning {{
      color:#7a4d00; background:linear-gradient(180deg,#fff7e6,#fff3cd);
      border-color:#ff9800; box-shadow:0 0 0 3px rgba(255,152,0,.25), 0 0 18px rgba(255,152,0,.45);
    }}
    .status-danger {{
      color:#7f1d1d; background:linear-gradient(180deg,#fdecea,#fde0dc);
      border-color:#e53935; box-shadow:0 0 0 3px rgba(229,57,53,.25), 0 0 20px rgba(229,57,53,.55);
    }}
  </style>

  <div class="alerts-header">âš ï¸ <h3>Alerts</h3></div>
  <h2 style="font-size: 24px; color: #333; margin: 10px 0 16px;">Active alerts</h2>

  <div class="alert-list" id="alertList">
    {''.join(cards_html)}
  </div>

  {''.join(modals_html)}
</div>
"""
print("ALERTS_HTML rebuilt: Details modal now shows an English glowing station-status box.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import numpy as np
import pandas as pd
from urllib.parse import quote
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# === 1. ×§×¨×™××” ××”-GitHub (×©×™× ×•×™ ×©××•×ª ×‘×œ×‘×“ ×œ×× ×™×¢×ª ×”×ª× ×’×©×•×ª) ===
ML_USER   = "NahlaAboromi"
ML_REPO   = "ecological-modeling-lab-team-biodynamics"
ML_BRANCH = "main"

ML_FILES = {
    "meto":     "Meto_Zemah2024_hourly (2).csv",
    "waves":    "KNW_ALL_Waves_merged (1).csv",
    "currents": "KNW_ALL_Currents_merged (2)_hourly_recalculated.csv",
}

def ml_raw_url(path):
    return f"https://raw.githubusercontent.com/{ML_USER}/{ML_REPO}/{ML_BRANCH}/{quote(path)}"

meto     = pd.read_csv(ml_raw_url(ML_FILES["meto"]))
waves    = pd.read_csv(ml_raw_url(ML_FILES["waves"]))
currents = pd.read_csv(ml_raw_url(ML_FILES["currents"]))

# === 2. ×”××¨×ª ×¢××•×“×ª ×”×–××Ÿ ×œ×¨×–×•×œ×•×¦×™×™×ª ×©×¢×” ××—×™×“×” ===
def coerce_time(df, candidates=("time","date","Date","Time")):
    for c in candidates:
        if c in df.columns:
            out = df.copy()
            out["time"] = pd.to_datetime(out[c], errors="coerce", dayfirst=True)  # ×œ×¤×™ ×”×¡×’× ×•×Ÿ ×©×œ×š
            out = out.dropna(subset=["time"])
            out["time"] = out["time"].dt.floor("h")
            return out
    raise ValueError("×œ× × ××¦××” ×¢××•×“×ª ×–××Ÿ ××ª××™××” (time/date).")

meto     = coerce_time(meto)
waves    = coerce_time(waves)
currents = coerce_time(currents)

# === 3. ×‘×—×™×¨×ª ×¢××•×“×•×ª ×¨×œ×•×•× ×˜×™×•×ª ×•×—×™×©×•×‘ U/V wind ===
meto = meto[["time","Ws_Z","Wd_Z"]].drop_duplicates(subset=["time"])
meto["Wd_Z_rad"] = np.deg2rad(meto["Wd_Z"])
meto["U_wind"]   = meto["Ws_Z"] * np.sin(meto["Wd_Z_rad"])
meto["V_wind"]   = meto["Ws_Z"] * np.cos(meto["Wd_Z_rad"])

waves = waves[["time","Hs (m)"]].drop_duplicates(subset=["time"])
currents = currents[["time","Velocity E [cm/s]","Velocity N [cm/s]","Magnitude [cm/s]"]].drop_duplicates(subset=["time"])

# === 4. ××™×—×•×“ ×¨×§ ×œ×©×¢×•×ª ×”××©×•×ª×¤×•×ª ===
common = set(meto["time"]).intersection(waves["time"]).intersection(currents["time"])
df = (meto[meto["time"].isin(common)]
      .merge(waves[waves["time"].isin(common)], on="time", how="inner")
      .merge(currents[currents["time"].isin(common)], on="time", how="inner")
      .sort_values("time").reset_index(drop=True))

print(f"âœ… × ××¦××• {len(df)} ×©×¢×•×ª ××©×•×ª×¤×•×ª")
print(f"×˜×•×•×—: {df['time'].min()} â†’ {df['time'].max()}")
print(df.head())

# === 5. ×”×›× ×” ×œ××•×“×œ ===
# ×œ×“×•×’××”: ×œ×—×–×•×ª Hs ×¢×œ ×¡××š U/V_wind
X = df[["U_wind","V_wind"]]          # ×¤×™×¦'×¨×™×
y = df["Hs (m)"]                     # ××©×ª× ×” ×™×¢×“

# ×¤×™×¦×•×œ Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ====== ML Forecast API (wrap existing cells; no logic change) ======

# ---- Hs forecast (delta-learning) ----
def ML_forecast_hs_fig(base_time_str="2024-07-10 10:00",
                       horizons=range(1, 11),
                       lags=list(range(0, 7)),
                       roll_wins=[3, 6],
                       model_name="KNN Regressor",
                       n_hours=10):
    # ××©×ª××©×™× ×‘×“×™×•×§ ×‘××•×ª×• ×§×•×“ ×©×œ×š, ×¨×§ ×‘×ª×•×š ×¤×•× ×§×¦×™×”
    from sklearn.preprocessing import StandardScaler
    from sklearn.neighbors import KNeighborsRegressor
    import matplotlib.pyplot as plt
    import numpy as np, pandas as pd

    # × ×©×ª××© ×‘- df ×”×§×™×™× ×‘×“×™×•×§ ×›××• ××¦×œ×š
    ML_base_df = df.copy().sort_values("time").reset_index(drop=True)
    ML_TARGET = "Hs (m)"

    def ML_build_features(work, target):
        feats = []
        for lag in lags:
            for col in ["U_wind","V_wind", target]:
                c = f"{col}_lag{lag}"
                work[c] = work[col].shift(lag)
                feats.append(c)
        for w in roll_wins:
            for col in ["U_wind","V_wind", target]:
                m = f"{col}_roll{w}"
                s = f"{col}_std{w}"
                work[m] = work[col].rolling(w, min_periods=1).mean()
                work[s] = work[col].rolling(w, min_periods=1).std()
                feats += [m, s]
        return work, feats

    def ML_point_forecast_delta_no_leak(base_df, target, H, base_time):
        work = base_df.copy()
        work, feat_cols = ML_build_features(work, target)
        delta_col = f"delta_{target}_H{H}"
        work[delta_col] = work[target].shift(-H) - work[target]
        work = work.dropna(subset=feat_cols + [delta_col]).reset_index(drop=True)

        bt = pd.Timestamp(base_time)
        if bt in set(work["time"]):
            row_idx = work.index[work["time"] == bt][0]
        else:
            mask = work["time"] <= bt
            if not mask.any():
                raise ValueError("××™×Ÿ ×©×•×¨×•×ª ×¢×“ ×–××Ÿ ×”×‘×¡×™×¡.")
            row_idx = mask[mask].index.max()

        past = work["time"] < work.loc[row_idx, "time"]
        Xtr = work.loc[past, feat_cols].values
        ytr = work.loc[past, delta_col].values
        if len(Xtr) < 50:
            raise ValueError("××¢×˜ ××“×™ ×”×™×¡×˜×•×¨×™×” ×œ××™××•×Ÿ.")
        scaler = StandardScaler()
        Xtr_s = scaler.fit_transform(Xtr)
        model = KNeighborsRegressor(n_neighbors=5, weights="distance")
        model.fit(Xtr_s, ytr)

        x_now = work.loc[[row_idx], feat_cols].values
        y_now = float(work.loc[row_idx, target])
        d_hat = float(model.predict(scaler.transform(x_now))[0])
        y_hat = y_now + d_hat

        t_now = work.loc[row_idx, "time"]
        t_fut = t_now + pd.Timedelta(hours=H)
        return {"H": H, "t": t_now, "t+H": t_fut, "pred": y_hat}

    ML_rows = [ML_point_forecast_delta_no_leak(ML_base_df, ML_TARGET, H, base_time_str) for H in horizons]
    ML_res = pd.DataFrame(ML_rows)

    fig = plt.figure(figsize=(9, 4.5))
    plt.plot(ML_res["t+H"], ML_res["pred"], marker="o", linewidth=2, label="Forecast")
    base_str = pd.Timestamp(base_time_str).strftime("%Y-%m-%d %H:%M")
    plt.title(f"Wave height (Hs) â€” {model_name}\nPoint forecast for the next {n_hours} hours (base: {base_str})")
    plt.xlabel("Time"); plt.ylabel("Hs [m]")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout()
    return fig  # ×”×—×–×¨×ª ×”×¤×™×’×•×¨

# ---- Magnitude forecast (delta-learning) ----
def ML_forecast_magnitude_fig(base_time_str="2024-07-10 10:00",
                              horizons=range(1, 11),
                              lags=list(range(0, 7)),
                              roll_wins=[3, 6],
                              model_name="KNN Regressor",
                              n_hours=10):
    from sklearn.preprocessing import StandardScaler
    from sklearn.neighbors import KNeighborsRegressor
    import matplotlib.pyplot as plt
    import numpy as np, pandas as pd

    ML_base_df = df.copy().sort_values("time").reset_index(drop=True)
    ML_TARGET = "Magnitude [cm/s]"

    def ML_build_features_curr(work, target):
        feats = []
        base_cols = ["U_wind","V_wind","Hs (m)","Velocity E [cm/s]","Velocity N [cm/s]", target]
        for lag in lags:
            for col in base_cols:
                c = f"{col}_lag{lag}"
                work[c] = work[col].shift(lag)
                feats.append(c)
        for w in roll_wins:
            for col in base_cols:
                m = f"{col}_roll{w}"; s = f"{col}_std{w}"
                work[m] = work[col].rolling(w, min_periods=1).mean()
                work[s] = work[col].rolling(w, min_periods=1).std()
                feats += [m, s]
        work["dU"] = work["U_wind"].diff()
        work["dV"] = work["V_wind"].diff()
        feats += ["dU", "dV"]
        return work, feats

    def ML_point_forecast_delta_no_leak_curr(base_df, target, H, base_time):
        work = base_df.copy()
        work, feat_cols = ML_build_features_curr(work, target)
        delta_col = f"delta_{target}_H{H}"
        work[delta_col] = work[target].shift(-H) - work[target]
        work = work.dropna(subset=feat_cols + [delta_col]).reset_index(drop=True)

        bt = pd.Timestamp(base_time)
        if bt in set(work["time"]):
            row_idx = work.index[work["time"] == bt][0]
        else:
            mask = work["time"] <= bt
            if not mask.any():
                raise ValueError("No rows up to base time.")
            row_idx = mask[mask].index.max()

        past = work["time"] < work.loc[row_idx, "time"]
        Xtr = work.loc[past, feat_cols].values
        ytr = work.loc[past, delta_col].values
        if len(Xtr) < 50:
            raise ValueError("Not enough history for training.")
        scaler = StandardScaler()
        Xtr_s = scaler.fit_transform(Xtr)
        model = KNeighborsRegressor(n_neighbors=5, weights="distance")
        model.fit(Xtr_s, ytr)

        x_now = work.loc[[row_idx], feat_cols].values
        y_now = float(work.loc[row_idx, target])
        d_hat = float(model.predict(scaler.transform(x_now))[0])
        y_hat = y_now + d_hat

        t_now = work.loc[row_idx, "time"]
        t_fut = t_now + pd.Timedelta(hours=H)
        return {"H": H, "t": t_now, "t+H": t_fut, "pred": y_hat}

    ML_rows = [ML_point_forecast_delta_no_leak_curr(ML_base_df, ML_TARGET, H, base_time_str) for H in horizons]
    ML_res = pd.DataFrame(ML_rows)

    fig = plt.figure(figsize=(9, 4.5))
    plt.plot(ML_res["t+H"], ML_res["pred"], marker="o", linewidth=2, label="Forecast")
    base_str = pd.Timestamp(base_time_str).strftime("%Y-%m-%d %H:%M")
    plt.title(f"Current magnitude â€” {model_name}\nPoint forecast for the next {n_hours} hours (base: {base_str})")
    plt.xlabel("Time"); plt.ylabel("Magnitude [cm/s]")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout()
    return fig

# ===== WRAP: Magnitude delta-forecast into callable functions (no logic change) =====
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor

# ××•×ª× ×‘×¨×™×¨×•×ª ××—×“×œ ×‘×“×™×•×§
ML_BASE_TIME_STR = "2024-07-10 10:00"
ML_TARGET = "Magnitude [cm/s]"
ML_HORIZONS = range(1, 11)
ML_LAGS = list(range(0, 7))   # memory up to 6h
ML_ROLL_WINS = [3, 6]         # rolling windows
ML_MODEL_NAME = "KNN Regressor"
ML_N_HOURS = 10

# ××©×ª××© ×‘-df ×”×’×œ×•×‘×œ×™ ×©×œ×š, ×œ×œ× ×©×™× ×•×™
ML_base_df = df.copy().sort_values("time").reset_index(drop=True)

def ML_build_features_curr(work, target):
    feats = []
    base_cols = ["U_wind","V_wind","Hs (m)","Velocity E [cm/s]","Velocity N [cm/s]", target]
    # Lags
    for lag in ML_LAGS:
        for col in base_cols:
            c = f"{col}_lag{lag}"
            work[c] = work[col].shift(lag)
            feats.append(c)
    # Rolling mean/std
    for w in ML_ROLL_WINS:
        for col in base_cols:
            m = f"{col}_roll{w}"
            s = f"{col}_std{w}"
            work[m] = work[col].rolling(w, min_periods=1).mean()
            work[s] = work[col].rolling(w, min_periods=1).std()
            feats += [m, s]
    # Wind derivatives
    work["dU"] = work["U_wind"].diff()
    work["dV"] = work["V_wind"].diff()
    feats += ["dU", "dV"]
    return work, feats

def ML_point_forecast_delta_no_leak_curr(base_df, target, H, base_time):
    work = base_df.copy()
    work, feat_cols = ML_build_features_curr(work, target)

    # Delta target: Î” = y(t+H) - y(t)
    delta_col = f"delta_{target}_H{H}"
    work[delta_col] = work[target].shift(-H) - work[target]

    # Drop NaNs from shifts/rollings
    work = work.dropna(subset=feat_cols + [delta_col]).reset_index(drop=True)

    # Locate base time
    bt = pd.Timestamp(base_time)
    if bt in set(work["time"]):
        row_idx = work.index[work["time"] == bt][0]
    else:
        mask = work["time"] <= bt
        if not mask.any():
            raise ValueError("No rows up to base time.")
        row_idx = mask[mask].index.max()

    # Train on past only (< t)
    past = work["time"] < work.loc[row_idx, "time"]
    Xtr = work.loc[past, feat_cols].values
    ytr = work.loc[past, delta_col].values
    if len(Xtr) < 50:
        raise ValueError("Not enough history for training.")

    scaler = StandardScaler()
    Xtr_s = scaler.fit_transform(Xtr)
    model = KNeighborsRegressor(n_neighbors=5, weights="distance")
    model.fit(Xtr_s, ytr)

    # Predict Î” then convert to absolute yÌ‚(t+H) = y(t) + Î”Ì‚
    x_now = work.loc[[row_idx], feat_cols].values
    y_now = float(work.loc[row_idx, target])
    d_hat = float(model.predict(scaler.transform(x_now))[0])
    y_hat = y_now + d_hat

    t_now = work.loc[row_idx, "time"]
    t_fut = t_now + pd.Timedelta(hours=H)

    return {"H": H, "t": t_now, "t+H": t_fut, "pred": y_hat}

# --- ××—×–×™×¨ DataFrame ×©×œ ×”×ª×—×–×™×ª (×œ×§×¨×™××” ××”×“×©×‘×•×¨×“)
def ML_forecast_magnitude_df(base_time_str=ML_BASE_TIME_STR,
                             horizons=ML_HORIZONS):
    rows = [ML_point_forecast_delta_no_leak_curr(ML_base_df, ML_TARGET, H, base_time_str) for H in horizons]
    return pd.DataFrame(rows)

# --- ××—×–×™×¨ Figure (××•×ª×• ×’×¨×£ ×›××• ×‘×ª× ×”××§×•×¨×™) â€” ××ª××™× ×œ-gr.Plot
def ML_forecast_magnitude_fig(base_time_str=ML_BASE_TIME_STR,
                              horizons=ML_HORIZONS,
                              model_name=ML_MODEL_NAME,
                              n_hours=ML_N_HOURS):
    res = ML_forecast_magnitude_df(base_time_str, horizons)
    fig = plt.figure(figsize=(9, 4.5))
    plt.plot(res["t+H"], res["pred"], marker="o", linewidth=2, label="Forecast")
    base_str = pd.Timestamp(base_time_str).strftime("%Y-%m-%d %H:%M")
    plt.title(f"Current magnitude â€” {model_name}\nPoint forecast for the next {n_hours} hours (base: {base_str})")
    plt.xlabel("Time"); plt.ylabel("Magnitude [cm/s]")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout()
    return fig

# ===== Gradio App: Home + Overview + Simulation + Map + Alerts + RAG + Forecast (single-line navbar) =====
import gradio as gr
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ----- existing logic you already have -----
labels = get_common_labels()
prefer_label = "10/06/2024 13:10"
default_label = prefer_label if prefer_label in labels else (labels[0] if labels else None)

def _run_method(selected_label, method):
    if not selected_label:
        return None, None
    ts = label_to_timestamp(selected_label)
    return (fig_ok(ts), fig_ok_variance(ts)) if method == "OK" else (fig_uk(ts), fig_uk_variance(ts))

pressure_labels = ["03/07/2024 08:30", "21/02/2024 08:10", "23/01/2024 08:30"]
baseline_labels  = ["27/02/2024 16:30", "29/02/2024 16:20"]
recovery_labels_preferred = ["23/01/2024 15:10", "23/01/2024 15:20", "23/01/2024 15:30"]
labels_all = get_common_labels()
recovery_labels = [lab for lab in recovery_labels_preferred if lab in labels_all]
scenario_options = {"Pressure": pressure_labels, "Baseline": baseline_labels, "Recovery": recovery_labels}

def _ts_from_label(lab: str):
    try:
        return label_to_timestamp(lab)
    except Exception:
        return pd.to_datetime(lab, dayfirst=True).floor("10min")

def _render_ok_uk(selected_label: str):
    if not selected_label:
        return None, None
    ts = _ts_from_label(selected_label)
    return fig_ok(ts), fig_uk(ts)

def _update_time_choices(scenario: str):
    opts = scenario_options.get(scenario, [])
    if not opts:
        return gr.update(choices=[], value=None, interactive=False), None, None
    default_lbl = opts[0]
    ok_fig, uk_fig = _render_ok_uk(default_lbl)
    return gr.update(choices=opts, value=default_lbl, interactive=True), ok_fig, uk_fig

def _render_from_inputs(scenario: str, label: str):
    if not label:
        return None, None
    return _render_ok_uk(label)

# ----- GLOBAL CSS: navbar + home alerts/button -----
APP_CSS = """
/* === FIX: allow vertical scrolling and wrapping on short/small screens === */
html, body, .gradio-container {
  height: auto !important;
  overflow-y: auto !important;   /* enable vertical scroll */
}
.gradio-container .block,
.gradio-container .wrap,
.gradio-container .grid-wrap {
  overflow: visible !important;  /* prevent hidden inner scroll areas */
}
/* Two-column area should wrap to a new line when narrow/short */
.two-col {
  display:flex; gap:16px; align-items:stretch; flex-wrap:wrap;  /* key: wrap */
}
.two-col > * {
  flex: 1 1 420px;   /* grow/shrink; min logical width so plots stack on small viewports */
}

/* ===== existing styles ===== */
.gradio-container, body { background:#ffffff !important; }

/* ===== NAV ===== */
#nav{ display:flex; align-items:center; gap:12px; padding:12px 16px; border-bottom:1px solid #eee; }
#nav .title{ font-weight:700; font-size:18px; flex:0 0 auto; white-space:nowrap; }
#nav-buttons{ display:flex; align-items:center; gap:10px; flex:1 1 auto; justify-content:flex-end; flex-wrap:nowrap; overflow:visible; min-width:0; }
.btn-tab{ border:1px solid #ddd; background:#f7f7f7; padding:8px 12px; border-radius:8px; display:flex; align-items:center; justify-content:center; text-align:center; line-height:1.1; white-space:nowrap; min-width:120px; height:44px; flex:0 0 auto; }
.btn-tab:hover{ background:#efefef; }

.info-card{ margin:10px 0 14px; background:linear-gradient(180deg,#f7fbff 0%,#ffffff 45%); border:1px solid #e6eef6; border-left:4px solid #1e88e5; border-radius:12px; padding:12px 14px; box-shadow:0 2px 8px rgba(31,41,55,.06); }
.info-card h4{ margin:0 0 6px; font-size:15px; font-weight:800; color:#1b3a57; }
.info-card p{ margin:0; color:#40576d; }
.info-card ul{ margin:6px 0 0; padding-left:18px; color:#40576d; }
.info-card li{ margin:4px 0; }
.callout-green{ margin-top:8px; background:#eefaf1; border:1px solid #c6f1d6; border-left:4px solid #2e7d32; color:#1b5e20; border-radius:10px; padding:10px 12px; }

.app-header{ background:linear-gradient(90deg,#274060 0%,#3a6ea5 50%,#2d9cdb 100%); padding:26px 20px; border-radius:18px; margin-bottom:14px; text-align:center; box-shadow:0 4px 12px rgba(0,0,0,.15); }
.app-header .app-title{ color:#fff !important; font-size:1.8rem; font-weight:800; margin:0; text-shadow:0 1px 1px rgba(0,0,0,.18); }
.app-header .app-sub{ color:#fff !important; opacity:.95; margin-top:8px; font-size:1rem; }

.cards{ display:grid; grid-template-columns:repeat(3,minmax(0,1fr)); gap:16px; margin-top:18px; }
@media (max-width:1100px){ .cards{ grid-template-columns:1fr; } }
.card{ background:#fff; border:1px solid #e6eef6; border-radius:16px; padding:16px; box-shadow:0 6px 16px rgba(31,41,55,.08); }
.card h4{ margin:0; font-weight:700; color:#2a3b4c; display:flex; align-items:center; gap:8px; }
.metric{ font-size:30px; font-weight:800; line-height:1; margin-top:8px; display:flex; gap:6px; }

.status{ margin:18px 0 0 0; background:#e9f7ef; border:1px solid #c9ecd9; color:#1f7a4f; border-radius:12px; padding:10px 14px; display:flex; align-items:center; gap:8px; }
.status.advisory{ background:#fff7ed; border-color:#fed7aa; color:#9a3412; }
.dot{ width:10px; height:10px; background:#22c55e; border-radius:999px; display:inline-block; }
.dot.warn{ background:#f59e0b; }

/* ===== Alerts row ===== */
#alerts-row{ display:flex; align-items:center; gap:12px; margin-top:16px; flex-wrap:nowrap; width:100%; justify-content:flex-start; }
#alerts-row > div{ flex:0 0 auto !important; width:auto !important; display:inline-flex !important; align-items:center; }

/* Yellow alerts pill */
.alerts-bar{ display:inline-flex; align-items:center; gap:10px; padding:10px 14px; height:40px; border-radius:12px; background:#fff8e6; border:1px solid #fbd38d; border-left:6px solid #f59e0b; width:auto; white-space:nowrap; }
.alerts-bar.ok{ background:#eefaf1; border-color:#b7ebc6; border-left-color:#2e7d32; }
.alerts-bar .hint{ color:#6b7a90; font-size:0.95rem; }
.alerts-bar .count{ font-weight:800; }

/* ===== View alerts button ===== */
#view-alerts-btn{ display:inline-flex !important; width:auto !important; }
#view-alerts-btn button,
#view-alerts-btn .gr-button,
#view-alerts-btn [type="button"]{
  width:auto !important; min-width:unset !important;
  padding:10px 16px; height:40px; line-height:20px;
  background:#0d47a1 !important; color:#fff !important;
  border:1px solid #093170 !important; border-radius:12px !important;
  font-weight:700; font-size:0.9rem;
  box-shadow:0 1px 2px rgba(0,0,0,.06);
  display:inline-flex; align-items:center; justify-content:center; cursor:pointer;
}
#view-alerts-btn button:hover,
#view-alerts-btn .gr-button:hover,
#view-alerts-btn [type="button"]{
  background:#093b87 !important; border-color:#072a64 !important; color:#fff !important;
}
"""
plt.ioff()
plt.close('all')

with gr.Blocks(title="Smart Kinneret â€” Dashboard", css=APP_CSS) as app:
    # NAVBAR (single line)  **Forecast before Alerts and RAG**
    with gr.Row(elem_id="nav"):
        gr.Markdown("ğŸŒŠ **Smart Kinneret Dashboard**", elem_classes=["title"])
        with gr.Row(elem_id="nav-buttons"):
            nav_home     = gr.Button("Home", elem_classes=["btn-tab"])
            nav_overview = gr.Button("All Simulations", elem_classes=["btn-tab"])
            nav_sim      = gr.Button("Specific Simulations", elem_classes=["btn-tab"])
            nav_map      = gr.Button("Stations Map", elem_classes=["btn-tab"])
            nav_forecast = gr.Button("Forecast", elem_classes=["btn-tab"])  # â† moved up
            nav_alerts   = gr.Button("Alerts", elem_classes=["btn-tab"])
            nav_rag      = gr.Button("RAG", elem_classes=["btn-tab"])

    # ===== PAGES =====
    page_home, btn_home_alerts = build_home_tab()  # unchanged

    # ===== All Simulations =====
    with gr.Group(visible=False) as page_overview:
        gr.HTML(header_bar(
            title="ğŸŒ All Simulations â€” Select Time and Method",
            subtitle="Pick a timestamp and interpolation method (OK / UK)"
        ))
        gr.HTML("""
        <div class="info-card">
          <h4>Whatâ€™s on this page</h4>
          <p>This view maps the <b>U_wind</b> field (eastâ€“west wind component, m/s) over Lake Kinneret at the selected time using Kriging.</p>
          <ul>
            <li><b>Method</b>: <b>OK</b> assumes a constant mean and is good for local patterns near stations; <b>UK</b> adds a basin-scale drift and often fits Kinneretâ€™s westâ†’east breeze gradient.</li>
            <li><b>Kriging Field</b> (left): interpolated U_wind over the lake.</li>
            <li><b>Variance</b> (right): prediction uncertainty â€” <i>lower</i> near stations, <i>higher</i> at edges; treat high-variance areas with caution.</li>
            <li>Switch between OK/UK to compare how the large-scale gradient is captured vs. local â€œblobsâ€ around stations.</li>
          </ul>
          <div class="callout-green">Station markers: the <b>4 dots</b> indicate the locations with real measurements. <b>Everywhere else</b> the surface is a <b>model prediction (Kriging)</b>.</div>
        </div>
        """)
        with gr.Row():
            dd_time   = gr.Dropdown(choices=labels, value=default_label, label="Select Time (dd/mm/yyyy HH:MM)")
            dd_method = gr.Dropdown(choices=["OK", "UK"], value="OK", label="Select Method")
        with gr.Row(elem_classes=["two-col"]):
            out_field = gr.Plot(label="Kriging Field")
            out_var   = gr.Plot(label="Variance")
        dd_time.change(_run_method,   inputs=[dd_time, dd_method], outputs=[out_field, out_var])
        dd_method.change(_run_method, inputs=[dd_time, dd_method], outputs=[out_field, out_var])

    # ===== Specific Simulations =====
    with gr.Group(visible=False) as page_sim:
        gr.HTML(header_bar(
            title="ğŸ§ª Specific Simulations â€” choose scenario & date",
            subtitle="Pressure / Baseline / Recovery presets"
        ))
        gr.HTML("""
        <div class="info-card">
          <h4>Whatâ€™s on this page</h4>
          <p>Select a ready-made time window to compare <b>OK</b> and <b>UK</b> maps side-by-side for the same timestamp.</p>
          <ul>
            <li><b>Baseline</b>: calm/typical conditions used as reference.</li>
            <li><b>Pressure</b>: disturbed/irregular winds (system under stress).</li>
            <li><b>Recovery</b>: conditions trending back toward baseline.</li>
            <li>Compare textures: <b>OK</b> emphasizes local station influence; <b>UK</b> emphasizes a coherent basin-scale drift that often represents U_wind on Kinneret afternoons.</li>
          </ul>
          <div class="callout-green">Station markers: the <b>4 dots</b> indicate the locations with real measurements. <b>Everywhere else</b> the surface is a <b>model prediction (Kriging)</b>.</div>
        </div>
        """)
        with gr.Row():
            sim_scenario = gr.Dropdown(choices=["Pressure", "Baseline", "Recovery"],
                                       value="Baseline", label="Scenario type")
            sim_time     = gr.Dropdown(choices=baseline_labels, value=baseline_labels[0], interactive=True,
                                       label="Select time (dd/mm/yyyy HH:MM)")
        with gr.Row(elem_classes=["two-col"]):
            sim_ok = gr.Plot(label="OK (Ordinary Kriging)")
            sim_uk = gr.Plot(label="UK (Universal Kriging)")
        sim_scenario.change(_update_time_choices, inputs=[sim_scenario], outputs=[sim_time, sim_ok, sim_uk])
        sim_time.change(_render_from_inputs,      inputs=[sim_scenario, sim_time], outputs=[sim_ok, sim_uk])

    # ===== Stations Map =====
    page_map = build_map_tab()  # (visible=False inside)

    # ===== Alerts =====
    with gr.Group(visible=False) as page_alerts:
        gr.HTML(header_bar(title="âš ï¸ Alerts", subtitle="Wind / currents / waves â€” last 2 hours"))
        gr.HTML(ALERTS_HTML)

    # ===== RAG =====
    page_rag = build_rag_tab()  # (visible=False inside)

    # ===== NEW: Forecast page (fixed timestamp, no controls) =====
    FORECAST_FIXED_LABEL = "10/07/2024 10:00"
    _fixed_ts = _ts_from_label(FORECAST_FIXED_LABEL)
    _fixed_ts_str = _fixed_ts.strftime("%Y-%m-%d %H:%M")
    _fig_hs  = ML_forecast_hs_fig(base_time_str=_fixed_ts_str)
    _fig_mag = ML_forecast_magnitude_fig(base_time_str=_fixed_ts_str)

    with gr.Group(visible=False) as page_forecast:
        gr.HTML(header_bar(
            title="ğŸ”® Forecast â€” short-term ML",
            subtitle=f"Base time: {FORECAST_FIXED_LABEL} (dd/mm/yyyy HH:MM)"
        ))
        gr.HTML("""
        <div class="info-card callout-green">
          <h4>Wave Height Forecast (Hs)</h4>
          <p>This chart shows the expected wave height for the next <b>10 hours</b> (in meters).<br>
          We only use measurements that happened <i>before</i> the base time at the top.<br>
          Tip: an upward line = stronger waves; a downward line = calmer water.</p>
        </div>
        """)
        out_hs_plot = gr.Plot(value=_fig_hs, label="Forecast â€” Hs (next 10 hours)")
        gr.HTML("""
        <div class="info-card callout-green">
          <h4>Current Speed Forecast (Magnitude)</h4>
          <p>This chart shows the expected water current speed for the next <b>10 hours</b> (in cm/s).<br>
          The forecast is based on past measurements up to the base time.<br>
          Tip: a higher value = stronger current; a drop in the line = current is weakening.</p>
        </div>
        """)
        out_mag_plot = gr.Plot(value=_fig_mag, label="Forecast â€” Magnitude (next 10 hours)")

    # ===== NAV handlers =====
    def _to_home():
        return (gr.update(visible=True),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False))

    def _to_overview(selected_label, method):
        field_fig, var_fig = _run_method(selected_label, method)
        return (gr.update(visible=False),
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                field_fig, var_fig)

    def _to_sim():
        dd_update, ok_fig, uk_fig = _update_time_choices("Baseline")
        return (gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                "Baseline", dd_update, ok_fig, uk_fig)

    def _to_map():
        return (gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False))

    def _to_alerts():
        return (gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=True),
                gr.update(visible=False),
                gr.update(visible=False))

    def _to_rag():
        return (gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=True),
                gr.update(visible=False))

    def _to_forecast():
        return (gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=False),
                gr.update(visible=True))

    # ===== Wire navbar buttons =====
    nav_home.click(_to_home, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])

    nav_overview.click(_to_overview, inputs=[dd_time, dd_method],
                       outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast, out_field, out_var])

    nav_sim.click(_to_sim, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast,
                                    sim_scenario, sim_time, sim_ok, sim_uk])

    nav_map.click(_to_map, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])

    nav_alerts.click(_to_alerts, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])

    nav_rag.click(_to_rag, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])

    nav_forecast.click(_to_forecast, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])

    # Home "View alerts" button
    btn_home_alerts.click(_to_alerts, outputs=[page_home, page_overview, page_sim, page_map, page_alerts, page_rag, page_forecast])
    # ===== STORAGE USAGE CHECK (runs before Gradio launches) =====
    import os, shutil
    
    def _dir_size_gb(path: str) -> float:
        total = 0
        for dp, dn, fn in os.walk(path):
            for f in fn:
                try:
                    fp = os.path.join(dp, f)
                    if os.path.isfile(fp):
                        total += os.path.getsize(fp)
                except Exception:
                    pass
        return total / (1024 ** 3)
    
    print("\nğŸ’¾ STORAGE USAGE CHECK (GB)")
    targets = [
        ("/data", "persistent data/models"),
        (os.path.expanduser("~/.cache"), "HF/pip cache"),
        ("/tmp", "temp files"),
        (".", "repo files"),
    ]
    total_est = 0.0
    for path, label in targets:
        try:
            size = _dir_size_gb(path)
            total_est += size
            print(f"ğŸ“ {path:<20} â‰ˆ {size:6.2f} GB  â€” {label}")
        except Exception as e:
            print(f"âš ï¸ Could not check {path}: {e}")
    
    print(f"â‰ˆ Estimated total (these dirs only): {total_est:.2f} GB\n")
    

    app.launch(debug=False)